{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Well Bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing / Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Importing pandas package\n",
    "\n",
    "# Set the maximum number of columns to display to None\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy as np # Importing numpy package\n",
    "\n",
    "from typing import Dict, Tuple, List, Union # Importing specific types from typing module\n",
    "\n",
    "import re # Importing regular expression package\n",
    "\n",
    "from src.database_manager import DatabricksOdbcConnector # Importing DatabricksOdbcConnector class from database_manager module\n",
    "\n",
    "from scipy.spatial.distance import cdist # Importing cdist function from scipy package\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Excel/csv into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('wellHeader_with_Cluster.csv',dtype={'ChosenID':str},parse_dates=['FirstProdDate','Comp_Dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming cluster column to bundle\n",
    "df_raw = df_raw.rename(columns={'cluster':'bundle'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by a specific column (e.g., 'bundle') in ascending order\n",
    "df_raw.sort_values(by='bundle', ascending=True, ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2514, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Defining Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_columns(df: pd.DataFrame, columns_to_move: List[str], reference_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reorders the columns of a dataframe by moving specified columns next to a reference column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe whose columns need to be reordered.\n",
    "    columns_to_move (List[str]): The names of the columns to move.\n",
    "    reference_column (str): The name of the column next to which the specified columns should be placed.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The dataframe with reordered columns.\n",
    "    \"\"\"\n",
    "    columns_order: List[str] = df.columns.tolist()  # Get current column order as a list\n",
    "    if not all(col in columns_order for col in columns_to_move) or reference_column not in columns_order:\n",
    "        raise ValueError(\"Specified columns must exist in the dataframe\")\n",
    "    \n",
    "    # Find the index of the reference column\n",
    "    ref_idx: int = columns_order.index(reference_column)\n",
    "    \n",
    "    # Remove the columns to move from their current positions\n",
    "    for col in columns_to_move:\n",
    "        columns_order.remove(col)\n",
    "    \n",
    "    # Insert the columns to move next to the reference column\n",
    "    for col in reversed(columns_to_move):\n",
    "        columns_order.insert(ref_idx + 1, col)\n",
    "    \n",
    "    # Reorder the dataframe columns\n",
    "    return df[columns_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Creating Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DSU columns names from Lease Name columns\n",
    "\n",
    "df_raw['DSU'] = df_raw['LeaseName'].apply(\n",
    "    lambda x: re.sub(r'[^a-zA-Z\\s]', ' ',  # Remove special characters, keep letters and spaces\n",
    "                     re.match(r'([^\\d]+)', str(x)).group(1) if pd.notna(x) and re.match(r'([^\\d]+)', str(x)) else str(x))  \n",
    "                    .strip()  # Strip leading/trailing spaces\n",
    ").replace(r'\\s+', ' ', regex=True)  # Collapse multiple spaces into a single space\n",
    "\n",
    "# Placing DSU next to LeaseName\n",
    "df_raw = reorder_columns(df=df_raw, columns_to_move=['DSU'], reference_column='LeaseName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Creating dataframes that have more than one unique bundles or DSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the same DSU has more than one unique bundle\n",
    "same_DSU_diffBundle_df = df_raw[df_raw.groupby(\"DSU\")[\"bundle\"].transform(\"nunique\") > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the same bundle has more than one unique DSU\n",
    "same_Bundle_diffDSU_df = df_raw[df_raw.groupby(\"bundle\")[\"DSU\"].transform(\"nunique\") > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Defining Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_heel_toe_mid_lat_lon(well_trajectory: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the heel, toe, and mid-point latitude/longitude for each ChosenID in the well trajectory DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    well_trajectory: pd.DataFrame\n",
    "        DataFrame containing well trajectory data, including 'ChosenID', 'md', 'latitude', and 'longitude'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        A DataFrame with 'ChosenID', 'Heel_Lat', 'Heel_Lon', 'Toe_Lat', 'Toe_Lon', 'Mid_Lat', 'Mid_Lon'.\n",
    "\n",
    "    Example:\n",
    "    >>> data = {\n",
    "    ...     \"ChosenID\": [1001, 1001, 1001, 1002, 1002],\n",
    "    ...     \"md\": [5000, 5100, 5200, 6000, 6100],\n",
    "    ...     \"latitude\": [31.388, 31.389, 31.387, 31.400, 31.401],\n",
    "    ...     \"longitude\": [-103.314, -103.315, -103.316, -103.318, -103.319]\n",
    "    ... }\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> extract_heel_toe_mid_lat_lon(df)\n",
    "       ChosenID  Heel_Lat  Heel_Lon  Toe_Lat  Toe_Lon  Mid_Lat  Mid_Lon\n",
    "    0     1001    31.388  -103.314   31.387  -103.316  31.3875 -103.315\n",
    "    1     1002    31.400  -103.318   31.401  -103.319  31.4005 -103.3185\n",
    "    \"\"\"\n",
    "    # Ensure the data is sorted by MD in ascending order\n",
    "    well_trajectory = well_trajectory.sort_values(by=[\"ChosenID\", \"md\"], ascending=True)\n",
    "\n",
    "    # Group by 'ChosenID' and extract heel/toe lat/lon\n",
    "    heel_toe_df = (\n",
    "        well_trajectory.groupby(\"ChosenID\")\n",
    "        .agg(\n",
    "            heel_lat=(\"latitude\", \"first\"),\n",
    "            heel_lon=(\"longitude\", \"first\"),\n",
    "            toe_lat=(\"latitude\", \"last\"),\n",
    "            toe_lon=(\"longitude\", \"last\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calculate midpoints\n",
    "    heel_toe_df[\"mid_Lat\"] = (heel_toe_df[\"heel_lat\"] + heel_toe_df[\"toe_lat\"]) / 2\n",
    "    heel_toe_df[\"mid_Lon\"] = (heel_toe_df[\"heel_lon\"] + heel_toe_df[\"toe_lon\"]) / 2\n",
    "\n",
    "    return heel_toe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(lat1: np.ndarray, lon1: np.ndarray, lat2: np.ndarray, lon2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Determine the relative direction of (lat2, lon2) with respect to (lat1, lon1).\n",
    "    \n",
    "    Parameters:\n",
    "    lat1, lon1: np.ndarray\n",
    "        Latitude and longitude of the first well.\n",
    "    lat2, lon2: np.ndarray\n",
    "        Latitude and longitude of the second well.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray\n",
    "        Array indicating the direction (e.g., North, South, East, West) of well B relative to well A.\n",
    "    \"\"\"\n",
    "    lat_diff = lat2 - lat1\n",
    "    lon_diff = lon2 - lon1\n",
    "\n",
    "    conditions = [\n",
    "        np.abs(lat_diff) > np.abs(lon_diff),\n",
    "        lat_diff > 0,\n",
    "        lon_diff > 0\n",
    "    ]\n",
    "\n",
    "    choices = [\"North\", \"South\", \"East\", \"West\"]\n",
    "    \n",
    "    return np.select(\n",
    "        [conditions[0] & conditions[1], conditions[0] & ~conditions[1], ~conditions[0] & conditions[2], ~conditions[0] & ~conditions[2]],\n",
    "        choices\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_drill_direction_vectorized(well_trajectories: Dict[str, pd.DataFrame], i_indices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized function to determine the drilling direction of multiple wells using NumPy operations.\n",
    "    \n",
    "    Parameters:\n",
    "    well_trajectories: Dict[str, pd.DataFrame]\n",
    "        Dictionary containing well trajectory data indexed by ChosenID.\n",
    "    i_indices: np.ndarray\n",
    "        Array of ChosenID whose drill directions need to be calculated.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray\n",
    "        Array containing \"EW\" (East-West) or \"NS\" (North-South) for each well.\n",
    "    \"\"\"\n",
    "    azimuth_values = np.array([well_trajectories[i][\"azimuth\"].mean() if not well_trajectories[i].empty else np.nan for i in i_indices])\n",
    "    \n",
    "    conditions = (45 <= azimuth_values) & (azimuth_values < 135) | (225 <= azimuth_values) & (azimuth_values < 315)\n",
    "    drill_directions = np.where(np.isnan(azimuth_values), \"Unknown\", np.where(conditions, \"EW\", \"NS\"))\n",
    "    \n",
    "    return drill_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_calculate_3D_distance_matrix(trajectories: Dict[str, pd.DataFrame], i_indices: np.ndarray, k_indices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Efficiently calculate the 3D distances between multiple well pairs using NumPy operations.\n",
    "    \n",
    "    Parameters:\n",
    "    well_trajectories: Dict[str, pd.DataFrame]\n",
    "        Dictionary containing well trajectory data indexed by well ID.\n",
    "    i_indices: np.ndarray\n",
    "        Array of indices representing the first well in each pair.\n",
    "    k_indices: np.ndarray\n",
    "        Array of indices representing the second well in each pair.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        - Horizontal distances between the well pairs.\n",
    "        - Vertical distances between the well pairs.\n",
    "        - 3D distances between the well pairs.\n",
    "    \"\"\"\n",
    "    mid_A = np.array([trajectories[i][[\"x\", \"y\", \"tvd\"]].mean().values for i in i_indices])\n",
    "    mid_B = np.array([trajectories[k][[\"x\", \"y\", \"tvd\"]].mean().values for k in k_indices])\n",
    "\n",
    "    vertical_distances = np.abs(mid_A[:, 2] - mid_B[:, 2])\n",
    "    mid_B[:, 2] = mid_A[:, 2]  # Align Well B to Well A’s TVD\n",
    "\n",
    "    horizontal_distances = np.linalg.norm(mid_A[:, :2] - mid_B[:, :2], axis=1)\n",
    "    total_3D_distances = np.sqrt(horizontal_distances**2 + vertical_distances**2)\n",
    "\n",
    "    return horizontal_distances, vertical_distances, total_3D_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_i_k_pairs(df: pd.DataFrame, trajectories: Union[Dict[str, pd.DataFrame], pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate the i_k_pairs DataFrame, computing horizontal and vertical distances, \n",
    "    3D distances, drilling directions, and relative directions between well pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    df: pd.DataFrame\n",
    "        DataFrame containing well metadata with:\n",
    "        - \"ChosenID\" (str): Unique well identifier.\n",
    "\n",
    "    trajectories: Union[Dict[str, pd.DataFrame], pd.DataFrame]\n",
    "        Either:\n",
    "        - A dictionary mapping well IDs (\"ChosenID\") to trajectory DataFrames.\n",
    "        - A single DataFrame containing all trajectory data (must have \"ChosenID\" column).\n",
    "        \n",
    "    Each trajectory DataFrame should include:\n",
    "    - \"md\" (float): Measured depth.\n",
    "    - \"tvd\" (float): True vertical depth.\n",
    "    - \"inclination\" (float): Inclination angle in degrees.\n",
    "    - \"azimuth\" (float): represents the drilling direction.\n",
    "    - \"latitude\" (float): Latitude values, define the geographical position.\n",
    "    - \"longitude\" (float): Longitude values, define the geographical position.\n",
    "    - \"x\" (float): X-coordinate in a Cartesian coordinate system.\n",
    "    - \"y\" (float): Y-coordinate in a Cartesian coordinate system.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        DataFrame containing pairs of wells (`i_uwi`, `k_uwi`) with their computed distances \n",
    "        and directional relationships.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Convert to dictionary if input is a DataFrame\n",
    "    if isinstance(trajectories, pd.DataFrame):\n",
    "        if \"ChosenID\" not in trajectories.columns:\n",
    "            raise ValueError(\"🚨 Error: Trajectory DataFrame must contain a 'ChosenID' column.\")\n",
    "        trajectories = {cid: group for cid, group in trajectories.groupby(\"ChosenID\")}\n",
    "\n",
    "    step1_time = time.time()\n",
    "    print(f\"✅ Step 1: Trajectory DataFrame conversion took {step1_time - start_time:.4f} seconds.\")\n",
    "\n",
    "    # Get unique ChosenIDs from df and validate existence in trajectories\n",
    "    chosen_ids = df[\"ChosenID\"].unique()\n",
    "    missing_ids = [cid for cid in chosen_ids if cid not in trajectories]\n",
    "\n",
    "    if missing_ids:\n",
    "        print(f\"⚠️ The following ChosenIDs do not exist in the trajectory data and will be excluded: {missing_ids}\")\n",
    "\n",
    "    df = df[df[\"ChosenID\"].isin(trajectories)]\n",
    "    \n",
    "    step2_time = time.time()\n",
    "    print(f\"✅ Step 2: ChosenID validation and filtering took {step2_time - step1_time:.4f} seconds.\")\n",
    "\n",
    "    # Generate i-k pair indices\n",
    "    n = len(df)\n",
    "    i_idx, k_idx = np.triu_indices(n, k=1)\n",
    "\n",
    "    i_uwi = df.iloc[i_idx][\"ChosenID\"].values\n",
    "    k_uwi = df.iloc[k_idx][\"ChosenID\"].values\n",
    "\n",
    "    step3_time = time.time()\n",
    "    print(f\"✅ Step 3: i-k pair generation took {step3_time - step2_time:.4f} seconds.\")\n",
    "\n",
    "    # 🚀 Optimized Heel/Toe Extraction (Vectorized)\n",
    "    heel_toe_df = pd.concat(\n",
    "        [extract_heel_toe_mid_lat_lon(trajectories[cid]) for cid in df['ChosenID'].unique()], ignore_index=True\n",
    "    )\n",
    "    heel_toe_dict = heel_toe_df.set_index(\"ChosenID\").to_dict(orient=\"index\")\n",
    "\n",
    "    step4_time = time.time()\n",
    "    print(f\"✅ Step 4 (Optimized): Heel/Toe extraction took {step4_time - step3_time:.4f} seconds.\")\n",
    "\n",
    "    # Efficiently extract values using vectorized lookups\n",
    "    heel_lat_i = np.array([heel_toe_dict[i][\"heel_lat\"] for i in i_uwi])\n",
    "    heel_lon_i = np.array([heel_toe_dict[i][\"heel_lon\"] for i in i_uwi])\n",
    "    toe_lat_k = np.array([heel_toe_dict[k][\"toe_lat\"] for k in k_uwi])\n",
    "    toe_lon_k = np.array([heel_toe_dict[k][\"toe_lon\"] for k in k_uwi])\n",
    "\n",
    "    step5_time = time.time()\n",
    "    print(f\"✅ Step 5: Heel/Toe dictionary lookup took {step5_time - step4_time:.4f} seconds.\")\n",
    "\n",
    "    # 🚀 Optimized Distance Calculation (Fully Vectorized)\n",
    "    horizontal_dist, vertical_dist, total_3D_dist = optimized_calculate_3D_distance_matrix(trajectories, i_uwi, k_uwi)\n",
    "\n",
    "    step6_time = time.time()\n",
    "    print(f\"✅ Step 6 (Optimized): Distance calculations took {step6_time - step5_time:.4f} seconds.\")\n",
    "\n",
    "    # Compute drill directions\n",
    "    drill_directions = calculate_drill_direction_vectorized(trajectories, i_uwi)\n",
    "\n",
    "    step7_time = time.time()\n",
    "    print(f\"✅ Step 7: Drill direction calculation took {step7_time - step6_time:.4f} seconds.\")\n",
    "\n",
    "    # Determine directional relationship\n",
    "    ward_of_i = get_direction(heel_lat_i, heel_lon_i, toe_lat_k, toe_lon_k)\n",
    "\n",
    "    step8_time = time.time()\n",
    "    print(f\"✅ Step 8: Directional relationship calculation took {step8_time - step7_time:.4f} seconds.\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        \"i_uwi\": i_uwi,\n",
    "        \"k_uwi\": k_uwi,\n",
    "        \"horizontal_dist\": horizontal_dist,\n",
    "        \"vertical_dist\": vertical_dist,\n",
    "        \"3D_ft_to_same\": total_3D_dist,\n",
    "        \"drill_direction\": drill_directions,\n",
    "        \"ward_of_i\": ward_of_i\n",
    "    })\n",
    "\n",
    "    total_time = time.time()\n",
    "    print(f\"🚀 Total Execution Time: {total_time - start_time:.4f} seconds.\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(well_A: pd.DataFrame, well_B: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the percentage overlap between two horizontal wellbores.\n",
    "    \n",
    "    Parameters:\n",
    "    well_A: pd.DataFrame\n",
    "        Well trajectory data for Well A, including 'MD' (Measured Depth) and 'Inclination'.\n",
    "    well_B: pd.DataFrame\n",
    "        Well trajectory data for Well B, including 'MD' (Measured Depth) and 'Inclination'.\n",
    "    \n",
    "    Returns:\n",
    "    float:\n",
    "        Percentage of overlap relative to the shorter lateral.\n",
    "    \"\"\"\n",
    "    if well_A.empty or well_B.empty:\n",
    "        return 0.0\n",
    "\n",
    "    start_A, end_A = well_A[\"MD\"].min(), well_A[\"MD\"].max()\n",
    "    start_B, end_B = well_B[\"MD\"].min(), well_B[\"MD\"].max()\n",
    "\n",
    "    overlap_start = max(start_A, start_B)\n",
    "    overlap_end = min(end_A, end_B)\n",
    "\n",
    "    if overlap_start >= overlap_end:\n",
    "        return 0.0\n",
    "\n",
    "    overlap_length = overlap_end - overlap_start\n",
    "    shorter_length = min(end_A - start_A, end_B - start_B)\n",
    "\n",
    "    return (overlap_length / shorter_length) * 100 if shorter_length > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\apoorva.saxena\\onedrive - sitio royalties\\desktop\\project - apoorva\\python\\parent_child_spacing\\src\\database_manager.py:85: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result_df = pd.read_sql(sql_query, self.connection)\n"
     ]
    }
   ],
   "source": [
    "databricks = DatabricksOdbcConnector()\n",
    "\n",
    "# Filtering only Horizontal wells and getting their apis\n",
    "chosen_ids = \", \".join(f\"'{id}'\" for id in df_raw[df_raw['HoleDirection']=='H']['ChosenID'].unique())\n",
    "\n",
    "try:\n",
    "    databricks.connect()\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        LEFT(uwi, 10) AS ChosenID, \n",
    "        station_md_uscust AS md, \n",
    "        station_tvd_uscust AS tvd,\n",
    "        inclination, \n",
    "        azimuth, \n",
    "        latitude, \n",
    "        longitude, \n",
    "        x_offset_uscust AS x, \n",
    "        y_offset_uscust AS y\n",
    "    FROM ihs_sp.well.well_directional_survey_station\n",
    "    WHERE LEFT(uwi, 10) IN ({chosen_ids}) and inclination >= 89\n",
    "    ORDER BY uwi, md;\n",
    "    \"\"\"\n",
    "\n",
    "    df_directional = databricks.execute_query(query)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    databricks.close_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
