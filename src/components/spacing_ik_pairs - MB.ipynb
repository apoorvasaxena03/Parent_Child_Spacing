{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Well Bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing / Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Importing pandas package\n",
    "\n",
    "# Set the maximum number of columns to display to None\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy as np # Importing numpy package\n",
    "\n",
    "from typing import Dict, Tuple, List, Union # Importing specific types from typing module\n",
    "\n",
    "import re # Importing regular expression package\n",
    "\n",
    "from src.database_manager import DatabricksOdbcConnector # Importing DatabricksOdbcConnector class from database_manager module\n",
    "from src.utils import reorder_columns # Importing reorder_columns function from utils module\n",
    "\n",
    "from scipy.spatial.distance import cdist # Importing cdist function from scipy package\n",
    "\n",
    "import time\n",
    "\n",
    "import pyproj # Importing pyproj package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Excel/csv into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of column names for the DataFrame\n",
    "header_colms = ['Well Name', 'Chosen ID', 'Lease Name', 'RSV_CAT', 'Bench', 'First Prod Date', 'Hole Direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_excel('MB Header.xlsx',dtype={'Chosen ID':str},parse_dates=['First Prod Date'], usecols=header_colms) # Reading an Excel file into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.rename(columns={\n",
    "    'Chosen ID':'ChosenID',\n",
    "    'Well Name':'WellName',\n",
    "    'RSV_CAT':'RES_CAT',\n",
    "    'Bench':'Landing_Zone',\n",
    "    'First Prod Date':'FirstProdDate',\n",
    "    'Hole Direction':'HoleDirection',\n",
    "    'Lease Name':'LeaseName'\n",
    "}, inplace=True) # Renaming columna in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['First Prod Date'] = pd.to_datetime(df_raw['FirstProdDate']) # Converting 'FirstProdDate' column to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7703, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Creating DSU Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DSU columns names from Lease Name columns\n",
    "\n",
    "df_raw['DSU'] = df_raw['LeaseName'].apply(\n",
    "    lambda x: re.sub(r'[^a-zA-Z\\s]', ' ',  # Remove special characters, keep letters and spaces\n",
    "                     re.match(r'([^\\d]+)', str(x)).group(1) if pd.notna(x) and re.match(r'([^\\d]+)', str(x)) else str(x))  \n",
    "                    .strip()  # Strip leading/trailing spaces\n",
    ").replace(r'\\s+', ' ', regex=True)  # Collapse multiple spaces into a single space\n",
    "\n",
    "# Placing DSU next to LeaseName\n",
    "df_raw = reorder_columns(df=df_raw, columns_to_move=['DSU'], reference_column='LeaseName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Defining Functions that is used in calculation for i-k pair dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_heel_toe_mid_lat_lon(well_trajectory: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the heel, toe, and mid-point latitude/longitude for each ChosenID in the well trajectory DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    well_trajectory: pd.DataFrame\n",
    "        DataFrame containing well trajectory data, including 'ChosenID', 'md', 'latitude', and 'longitude'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        A DataFrame with 'ChosenID', 'Heel_Lat', 'Heel_Lon', 'Toe_Lat', 'Toe_Lon', 'Mid_Lat', 'Mid_Lon'.\n",
    "\n",
    "    Example:\n",
    "    >>> data = {\n",
    "    ...     \"ChosenID\": [1001, 1001, 1001, 1002, 1002],\n",
    "    ...     \"md\": [5000, 5100, 5200, 6000, 6100],\n",
    "    ...     \"latitude\": [31.388, 31.389, 31.387, 31.400, 31.401],\n",
    "    ...     \"longitude\": [-103.314, -103.315, -103.316, -103.318, -103.319]\n",
    "    ... }\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> extract_heel_toe_mid_lat_lon(df)\n",
    "       ChosenID  Heel_Lat  Heel_Lon  Toe_Lat  Toe_Lon  Mid_Lat  Mid_Lon\n",
    "    0     1001    31.388  -103.314   31.387  -103.316  31.3875 -103.315\n",
    "    1     1002    31.400  -103.318   31.401  -103.319  31.4005 -103.3185\n",
    "    \"\"\"\n",
    "    # Ensure the data is sorted by MD in ascending order\n",
    "    well_trajectory = well_trajectory.sort_values(by=[\"ChosenID\", \"md\"], ascending=True)\n",
    "\n",
    "    # Group by 'ChosenID' and extract heel/toe lat/lon\n",
    "    heel_toe_df = (\n",
    "        well_trajectory.groupby(\"ChosenID\")\n",
    "        .agg(\n",
    "            heel_lat=(\"latitude\", \"first\"),\n",
    "            heel_lon=(\"longitude\", \"first\"),\n",
    "            toe_lat=(\"latitude\", \"last\"),\n",
    "            toe_lon=(\"longitude\", \"last\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calculate midpoints\n",
    "    heel_toe_df[\"mid_Lat\"] = np.round((heel_toe_df[\"heel_lat\"] + heel_toe_df[\"toe_lat\"]) / 2, 9)\n",
    "    heel_toe_df[\"mid_Lon\"] = np.round((heel_toe_df[\"heel_lon\"] + heel_toe_df[\"toe_lon\"]) / 2, 9)\n",
    "\n",
    "    return heel_toe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(lat1: np.ndarray, lon1: np.ndarray, lat2: np.ndarray, lon2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Determine the relative direction of (lat2, lon2) with respect to (lat1, lon1).\n",
    "    \n",
    "    Parameters:\n",
    "    lat1, lon1: np.ndarray\n",
    "        Latitude and longitude of the first well.\n",
    "    lat2, lon2: np.ndarray\n",
    "        Latitude and longitude of the second well.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray\n",
    "        Array indicating the direction (e.g., North, South, East, West) of well B relative to well A.\n",
    "    \"\"\"\n",
    "    lat_diff = lat1 - lat2\n",
    "    lon_diff = lon1 - lon2\n",
    "\n",
    "    conditions = [\n",
    "        np.abs(lat_diff) > np.abs(lon_diff), # Determines if movement is more North/South\n",
    "        lat_diff > 0, # B is South of A\n",
    "        lon_diff > 0  # B is West of A\n",
    "    ]\n",
    "\n",
    "    choices = [\"N\", \"S\", \"E\", \"W\"]\n",
    "    \n",
    "    return np.select(\n",
    "        [conditions[0] & conditions[1], # More movement in North/South direction & B is South of A\n",
    "         conditions[0] & ~conditions[1], # More movement in North/South direction & B is North of A\n",
    "         ~conditions[0] & conditions[2], # More movement in East/West direction & B is West of A\n",
    "         ~conditions[0] & ~conditions[2]], # More movement in East/West direction & B is East of A\n",
    "        choices\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_drill_direction_vectorized(well_trajectories: Dict[str, pd.DataFrame], i_indices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Optimized vectorized function to determine the drilling direction of multiple wells using NumPy operations.\n",
    "    \n",
    "    Parameters:\n",
    "    well_trajectories: Dict[str, pd.DataFrame]\n",
    "        Dictionary containing well trajectory data indexed by ChosenID.\n",
    "    i_indices: np.ndarray\n",
    "        Array of ChosenID whose drill directions need to be calculated.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray\n",
    "        Array containing \"EW\" (East-West) or \"NS\" (North-South) for each well.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 🚀 Precompute medians for all wells at once\n",
    "    all_data = pd.concat(well_trajectories.values(), keys=well_trajectories.keys()).reset_index(level=0)\n",
    "    azimuth_medians = all_data.groupby(\"level_0\")[\"azimuth\"].median().to_dict()\n",
    "\n",
    "    step1_time = time.time()\n",
    "    print(f\"✅ Step 1: Precomputed azimuth medians in {step1_time - start_time:.4f} seconds.\")\n",
    "\n",
    "    # 🚀 Fast lookup using NumPy\n",
    "    azimuth_values = np.array([azimuth_medians.get(i, np.nan) for i in i_indices])\n",
    "\n",
    "    step2_time = time.time()\n",
    "    print(f\"✅ Step 2: Retrieved azimuth values in {step2_time - step1_time:.4f} seconds.\")\n",
    "\n",
    "    # 🚀 Apply vectorized conditions\n",
    "    conditions = (45 <= azimuth_values) & (azimuth_values < 135) | (225 <= azimuth_values) & (azimuth_values < 315)\n",
    "    drill_directions = np.where(np.isnan(azimuth_values), \"Unknown\", np.where(conditions, \"EW\", \"NS\"))\n",
    "\n",
    "    step3_time = time.time()\n",
    "    print(f\"✅ Step 3: Assigned drill directions in {step3_time - step2_time:.4f} seconds.\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"🚀 Total Execution Time: {total_time:.4f} seconds.\")\n",
    "\n",
    "    return drill_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_calculate_3D_distance_matrix(\n",
    "    trajectories: Dict[str, pd.DataFrame], i_indices: np.ndarray, k_indices: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Fully vectorized 3D distance calculations for well pairs using NumPy and Pandas.\n",
    "    \n",
    "    Parameters:\n",
    "    trajectories: Dict[str, pd.DataFrame]\n",
    "        Dictionary containing well trajectory data indexed by well ID.\n",
    "    i_indices: np.ndarray\n",
    "        Array of well IDs representing the first well in each pair.\n",
    "    k_indices: np.ndarray\n",
    "        Array of well IDs representing the second well in each pair.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        - Horizontal distances between the well pairs.\n",
    "        - Vertical distances between the well pairs.\n",
    "        - 3D distances between the well pairs.\n",
    "    \"\"\"\n",
    "    # 🚀 Precompute mean (midpoint) for each well ID across all wells at once\n",
    "    all_trajectories_df = pd.concat(trajectories.values(), keys=trajectories.keys()).reset_index(drop=True)\n",
    "\n",
    "    midpoints_df = all_trajectories_df.groupby(\"ChosenID\")[[\"x\", \"y\", \"tvd\"]].mean()\n",
    "\n",
    "    # Convert to NumPy arrays for fast lookup\n",
    "    well_ids = midpoints_df.index.to_numpy()\n",
    "    midpoints = midpoints_df.to_numpy()\n",
    "\n",
    "    # Create a mapping from well ID to its index\n",
    "    well_id_to_idx = {well_id: idx for idx, well_id in enumerate(well_ids)}\n",
    "\n",
    "    # Efficiently extract midpoints using NumPy indexing\n",
    "    mid_A = midpoints[np.array([well_id_to_idx[i] for i in i_indices])]\n",
    "    mid_B = midpoints[np.array([well_id_to_idx[k] for k in k_indices])]\n",
    "\n",
    "    # Compute distances\n",
    "    vertical_distances = np.abs(mid_A[:, 2] - mid_B[:, 2])\n",
    "    mid_B[:, 2] = mid_A[:, 2]  # Align Well B to Well A’s TVD\n",
    "\n",
    "    horizontal_distances = np.linalg.norm(mid_A[:, :2] - mid_B[:, :2], axis=1)\n",
    "    total_3D_distances = np.sqrt(horizontal_distances**2 + vertical_distances**2)\n",
    "\n",
    "    return horizontal_distances, vertical_distances, total_3D_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_i_k_pairs(df: pd.DataFrame, trajectories: Union[Dict[str, pd.DataFrame], pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate the i_k_pairs DataFrame, computing horizontal and vertical distances, \n",
    "    3D distances, drilling directions, and relative directions between well pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    df: pd.DataFrame\n",
    "        DataFrame containing well metadata with:\n",
    "        - \"ChosenID\" (str): Unique well identifier.\n",
    "\n",
    "    trajectories: Union[Dict[str, pd.DataFrame], pd.DataFrame]\n",
    "        Either:\n",
    "        - A dictionary mapping well IDs (\"ChosenID\") to trajectory DataFrames.\n",
    "        - A single DataFrame containing all trajectory data (must have \"ChosenID\" column).\n",
    "        \n",
    "    Each trajectory DataFrame should include:\n",
    "    - \"md\" (float): Measured depth.\n",
    "    - \"tvd\" (float): True vertical depth.\n",
    "    - \"inclination\" (float): Inclination angle in degrees.\n",
    "    - \"azimuth\" (float): represents the drilling direction.\n",
    "    - \"latitude\" (float): Latitude values, define the geographical position.\n",
    "    - \"longitude\" (float): Longitude values, define the geographical position.\n",
    "    - \"x\" (float): X-coordinate in a Cartesian coordinate system.\n",
    "    - \"y\" (float): Y-coordinate in a Cartesian coordinate system.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        DataFrame containing pairs of wells (`i_uwi`, `k_uwi`) with their computed distances \n",
    "        and directional relationships.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to dictionary if input is a DataFrame\n",
    "    step1_start = time.time()\n",
    "    if isinstance(trajectories, pd.DataFrame):\n",
    "        if \"ChosenID\" not in trajectories.columns:\n",
    "            raise ValueError(\"🚨 Error: Trajectory DataFrame must contain a 'ChosenID' column.\")\n",
    "        trajectories = {cid: group for cid, group in trajectories.groupby(\"ChosenID\")}\n",
    "    step1_end = time.time()\n",
    "    print(f\"✅ Step 1: Converted trajectory DataFrame to dictionary in {step1_end - step1_start:.4f} seconds.\")\n",
    "\n",
    "    # Get unique ChosenIDs from df\n",
    "    step2_start = time.time()\n",
    "    chosen_ids = df[\"ChosenID\"].unique()\n",
    "    missing_ids = [cid for cid in chosen_ids if cid not in trajectories]\n",
    "\n",
    "    if missing_ids:\n",
    "        print(f\"⚠️ The following ChosenIDs do not exist in the trajectory data and will be excluded: {missing_ids}\")\n",
    "\n",
    "    df = df[df[\"ChosenID\"].isin(trajectories)] # Filter out missing IDs in the DataFrame\n",
    "    chosen_ids = df[\"ChosenID\"].unique() # Update chosen_ids without missing IDs\n",
    "    step2_end = time.time()\n",
    "    print(f\"✅ Step 2: Extracted unique ChosenIDs in {step2_end - step2_start:.4f} seconds.\")\n",
    "\n",
    "    # Generate all possible pairs (excluding self-comparison)\n",
    "    step3_start = time.time()\n",
    "    i_uwi, k_uwi = np.meshgrid(chosen_ids, chosen_ids, indexing='ij')\n",
    "    i_uwi, k_uwi = i_uwi.ravel(), k_uwi.ravel()\n",
    "\n",
    "    # Remove self-comparisons\n",
    "    valid_mask = i_uwi != k_uwi\n",
    "    i_uwi, k_uwi = i_uwi[valid_mask], k_uwi[valid_mask]\n",
    "    step3_end = time.time()\n",
    "    print(f\"✅ Step 3: Generated well pairs in {step3_end - step3_start:.4f} seconds.\")\n",
    "\n",
    "    # 🚀 Optimized Heel/Toe Extraction (Vectorized)\n",
    "    step4_start = time.time()\n",
    "    heel_toe_df = pd.concat(\n",
    "        [extract_heel_toe_mid_lat_lon(trajectories[cid]) for cid in chosen_ids], ignore_index=True\n",
    "    )\n",
    "    heel_toe_dict = heel_toe_df.set_index(\"ChosenID\").to_dict(orient=\"index\")\n",
    "    step4_end = time.time()\n",
    "    print(f\"✅ Step 4: Heel/Toe extraction took {step4_end - step4_start:.4f} seconds.\")\n",
    "\n",
    "    # Efficiently extract values using vectorized lookups\n",
    "    step5_start = time.time()\n",
    "    \n",
    "    mid_lat_i = np.array([heel_toe_dict[i][\"mid_Lat\"] for i in i_uwi])\n",
    "    mid_lon_i = np.array([heel_toe_dict[i][\"mid_Lon\"] for i in i_uwi])\n",
    "    mid_lat_k = np.array([heel_toe_dict[k][\"mid_Lat\"] for k in k_uwi])\n",
    "    mid_lon_k = np.array([heel_toe_dict[k][\"mid_Lon\"] for k in k_uwi])\n",
    "    step5_end = time.time()\n",
    "    print(f\"✅ Step 5: Heel/Toe dictionary lookup took {step5_end - step5_start:.4f} seconds.\")\n",
    "\n",
    "    # 🚀 Optimized Distance Calculation (Fully Vectorized)\n",
    "    step6_start = time.time()\n",
    "    horizontal_dist, vertical_dist, total_3D_dist = optimized_calculate_3D_distance_matrix(trajectories, i_uwi, k_uwi)\n",
    "    step6_end = time.time()\n",
    "    print(f\"✅ Step 6: Distance calculations took {step6_end - step6_start:.4f} seconds.\")\n",
    "\n",
    "    # Compute drill directions\n",
    "    step7_start = time.time()\n",
    "    drill_directions = calculate_drill_direction_vectorized(trajectories, i_uwi)\n",
    "    step7_end = time.time()\n",
    "    print(f\"✅ Step 7: Drill direction calculation took {step7_end - step7_start:.4f} seconds.\")\n",
    "\n",
    "    # Determine directional relationship\n",
    "    step8_start = time.time()\n",
    "    ward_of_i = get_direction(mid_lat_i, mid_lon_i, mid_lat_k, mid_lon_k)\n",
    "    step8_end = time.time()\n",
    "    print(f\"✅ Step 8: Directional relationship calculation took {step8_end - step8_start:.4f} seconds.\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    step9_start = time.time()\n",
    "    result_df = pd.DataFrame({\n",
    "        \"i_uwi\": i_uwi,\n",
    "        \"k_uwi\": k_uwi,\n",
    "        \"horizontal_dist\": horizontal_dist,\n",
    "        \"vertical_dist\": vertical_dist,\n",
    "        \"3D_ft_to_same\": total_3D_dist,\n",
    "        \"drill_direction\": drill_directions,\n",
    "        \"ward_of_i\": ward_of_i\n",
    "    })\n",
    "    step9_end = time.time()\n",
    "    print(f\"✅ Step 9: Created result DataFrame in {step9_end - step9_start:.4f} seconds.\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"🚀 Total Execution Time: {total_time:.4f} seconds.\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(well_A: pd.DataFrame, well_B: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the percentage overlap between two horizontal wellbores.\n",
    "    \n",
    "    Parameters:\n",
    "    well_A: pd.DataFrame\n",
    "        Well trajectory data for Well A, including 'MD' (Measured Depth) and 'Inclination'.\n",
    "    well_B: pd.DataFrame\n",
    "        Well trajectory data for Well B, including 'MD' (Measured Depth) and 'Inclination'.\n",
    "    \n",
    "    Returns:\n",
    "    float:\n",
    "        Percentage of overlap relative to the shorter lateral.\n",
    "    \"\"\"\n",
    "    if well_A.empty or well_B.empty:\n",
    "        return 0.0\n",
    "\n",
    "    start_A, end_A = well_A[\"MD\"].min(), well_A[\"MD\"].max()\n",
    "    start_B, end_B = well_B[\"MD\"].min(), well_B[\"MD\"].max()\n",
    "\n",
    "    overlap_start = max(start_A, start_B)\n",
    "    overlap_end = min(end_A, end_B)\n",
    "\n",
    "    if overlap_start >= overlap_end:\n",
    "        return 0.0\n",
    "\n",
    "    overlap_length = overlap_end - overlap_start\n",
    "    shorter_length = min(end_A - start_A, end_B - start_B)\n",
    "\n",
    "    return (overlap_length / shorter_length) * 100 if shorter_length > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Defining Functions that is used to compute Lat/Lon to UTM Co-Ordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_utm_zone(longitude: float) -> int:\n",
    "    \"\"\"\n",
    "    Determines the UTM zone based on a given longitude.\n",
    "    \"\"\"\n",
    "    return int((longitude + 180) / 6) + 1\n",
    "\n",
    "\n",
    "def batch_latlon_to_utm(lat: np.ndarray, lon: np.ndarray, utm_zone: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts arrays of latitudes and longitudes to UTM coordinates in meters for a given UTM zone.\n",
    "    \"\"\"\n",
    "    proj_utm = pyproj.Transformer.from_crs(\n",
    "        \"EPSG:4326\", f\"EPSG:326{utm_zone}\", always_xy=True\n",
    "    )\n",
    "    \n",
    "    return proj_utm.transform(lon, lat)\n",
    "\n",
    "\n",
    "def compute_utm_coordinates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes UTM (x, y, z) coordinates for multiple wells, using surface location to determine UTM zones.\n",
    "    Converts UTM coordinates from meters to feet. Uses vectorized batch processing for performance.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Original directional survey DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with all original columns + x, y, z (in feet), and utm_zone.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Step 1: Sort dataframe by md to identify surface location\n",
    "    df = df.sort_values(by=[\"ChosenID\", \"md\"], ascending=[True, True])\n",
    "    \n",
    "    # Step 2: Determine UTM zones using the surface location (first row per well)\n",
    "    surface_locs = df.groupby(\"ChosenID\").first()[[\"latitude\", \"longitude\"]]\n",
    "    surface_locs[\"utm_zone\"] = surface_locs[\"longitude\"].apply(determine_utm_zone)\n",
    "\n",
    "    # Merge UTM zones back into the original dataframe\n",
    "    df = df.merge(surface_locs[[\"utm_zone\"]], on=\"ChosenID\", how=\"left\")\n",
    "\n",
    "    print(f\"✅ Determined UTM zones in {time.time() - start_time:.4f} seconds.\")\n",
    "\n",
    "    # Step 3: Batch transformation for each unique UTM zone\n",
    "    start_transform_time = time.time()\n",
    "    unique_zones = df[\"utm_zone\"].unique()\n",
    "    utm_converters: Dict[int, Tuple[np.ndarray, np.ndarray]] = {}\n",
    "\n",
    "    for zone in unique_zones:\n",
    "        subset = df[df[\"utm_zone\"] == zone]\n",
    "        easting, northing = batch_latlon_to_utm(subset[\"latitude\"].values, subset[\"longitude\"].values, zone)\n",
    "        utm_converters[zone] = (easting, northing)\n",
    "\n",
    "    print(f\"✅ Performed batch EPSG transformations in {time.time() - start_transform_time:.4f} seconds.\")\n",
    "\n",
    "    # Step 4: Assign the converted coordinates back to the DataFrame\n",
    "    start_assign_time = time.time()\n",
    "    df[\"x\"], df[\"y\"] = np.zeros(len(df)), np.zeros(len(df))\n",
    "\n",
    "    for zone in unique_zones:\n",
    "        mask = df[\"utm_zone\"] == zone\n",
    "        df.loc[mask, \"x\"], df.loc[mask, \"y\"] = utm_converters[zone]\n",
    "\n",
    "    print(f\"✅ Assigned transformed coordinates in {time.time() - start_assign_time:.4f} seconds.\")\n",
    "\n",
    "    # Step 5: Convert UTM coordinates from meters to feet (Conversion factor: 1 meter = 3.28084 feet)\n",
    "    df[\"x\"] *= 3.28084\n",
    "    df[\"y\"] *= 3.28084\n",
    "    \n",
    "    df[\"z\"] = -df[\"tvd\"] # Elevation is negative TVD\n",
    "\n",
    "    print(f\"✅ Total execution time: {time.time() - start_time:.4f} seconds.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_after_heel_point(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the dataframe to include all rows for each ChosenID where the first occurrence \n",
    "    of either '80' or 'heel' appears in the point_type column and all subsequent rows.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): A dataframe containing directional survey data with a 'ChosenID' column and 'point_type' column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered dataframe containing rows from the first occurrence of '80' or 'heel' onward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'point_type' to lowercase and check for '80' or 'heel'\n",
    "    mask = df['point_type'].str.lower().str.contains(r'80|heel', regex=True, na=False)\n",
    "\n",
    "    # Identify the first occurrence for each ChosenID\n",
    "    idx_start = df[mask].groupby('ChosenID', sort=False).head(1).index\n",
    "\n",
    "    # Create a mapping of ChosenID to the starting index\n",
    "    start_idx_map = dict(zip(df.loc[idx_start, 'ChosenID'], idx_start))\n",
    "\n",
    "    # Create a boolean mask using NumPy to filter rows\n",
    "    chosen_ids = df['ChosenID'].values\n",
    "    indices = np.arange(len(df))\n",
    "\n",
    "    # Get the minimum start index for each row's ChosenID\n",
    "    start_indices = np.vectorize(start_idx_map.get, otypes=[float])(chosen_ids)\n",
    "\n",
    "    # Mask rows where index is greater than or equal to the start index\n",
    "    valid_rows = indices >= start_indices\n",
    "\n",
    "    return df[valid_rows].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testinig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01PDP', '02PDNP', '02PA', 'OLD DUC', '03PUD', '03PA', '05PA'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['RES_CAT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering df_raw to include only certain RSV_CAT values\n",
    "df_raw = df_raw[df_raw['RES_CAT'].isin(['01PDP', '02PDNP', '03PUD'])].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\apoorva.saxena\\onedrive - sitio royalties\\desktop\\project - apoorva\\python\\parent_child_spacing\\src\\database_manager.py:85: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result_df = pd.read_sql(sql_query, self.connection)\n"
     ]
    }
   ],
   "source": [
    "# Importing Directional Survey data from Databricks\n",
    "\n",
    "databricks = DatabricksOdbcConnector()\n",
    "\n",
    "# Filtering only Horizontal wells and getting their apis\n",
    "chosen_ids = \", \".join(f\"'{id}'\" for id in df_raw[df_raw['HoleDirection']=='H']['ChosenID'].unique())\n",
    "\n",
    "try:\n",
    "    databricks.connect()\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        LEFT(uwi, 10) AS ChosenID, \n",
    "        station_md_uscust AS md, \n",
    "        station_tvd_uscust AS tvd,\n",
    "        inclination, \n",
    "        azimuth, \n",
    "        latitude, \n",
    "        longitude, \n",
    "        x_offset_uscust AS `deviation_E/W`,\n",
    "        ew_direction,\n",
    "        y_offset_uscust AS `deviation_N/S`,\n",
    "        ns_direction,\n",
    "        point_type\n",
    "        \n",
    "    FROM ihs_sp.well.well_directional_survey_station\n",
    "    WHERE LEFT(uwi, 10) IN ({chosen_ids})\n",
    "    order by uwi, md;\n",
    "    \"\"\"\n",
    "\n",
    "    df_directional = databricks.execute_query(query)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    databricks.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Determined UTM zones in 0.9981 seconds.\n",
      "✅ Performed batch EPSG transformations in 0.3482 seconds.\n",
      "✅ Assigned transformed coordinates in 0.0172 seconds.\n",
      "✅ Total execution time: 1.3836 seconds.\n"
     ]
    }
   ],
   "source": [
    "df_with_utm = compute_utm_coordinates(df_directional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filter_after_heel_point(df_with_utm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_heel_toe_mid_lat_lon(filtered_df[['ChosenID','md','tvd','inclination','azimuth','latitude','longitude','x','y','z']])\n",
    "# extract_heel_toe_mid_lat_lon(filtered_df[['ChosenID','md','tvd','inclination','azimuth','latitude','longitude','x','y','z']]).to_csv(r\"C:\\Users\\Apoorva.Saxena\\OneDrive - Sitio Royalties\\Desktop\\Project - Apoorva\\MB Investigation\\HeelToe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1: Converted trajectory DataFrame to dictionary in 0.3125 seconds.\n",
      "⚠️ The following ChosenIDs do not exist in the trajectory data and will be excluded: ['4232944094', '4232941762', '4232941872', '4246134423', '4222740685', '4222740681', '4232943669', '4222741542', '4246142022', '4246142016', '4222741585', '4231744184', '4232945684', '4238341181', '4222741952', '4246142273', '4246142272', '4231745076', '4232946465', '4231745686', '4231745690', '4231745685', '4246142636', '4246142644', '4231745868', '4231745970', '4231745971', '4232946926', '4231746124', '4232947036', '4231746232', '4231746283', '4231746288', '4232947200', '4231746448', '4246143025']\n",
      "✅ Step 2: Extracted unique ChosenIDs in 0.0050 seconds.\n",
      "✅ Step 3: Generated well pairs in 1.1121 seconds.\n",
      "✅ Step 4: Heel/Toe extraction took 19.9128 seconds.\n",
      "✅ Step 5: Heel/Toe dictionary lookup took 19.1727 seconds.\n",
      "✅ Step 6: Distance calculations took 11.9329 seconds.\n",
      "✅ Step 1: Precomputed azimuth medians in 0.2777 seconds.\n",
      "✅ Step 2: Retrieved azimuth values in 4.9224 seconds.\n",
      "✅ Step 3: Assigned drill directions in 0.6738 seconds.\n",
      "🚀 Total Execution Time: 5.8738 seconds.\n",
      "✅ Step 7: Drill direction calculation took 5.8980 seconds.\n",
      "✅ Step 8: Directional relationship calculation took 0.9911 seconds.\n",
      "✅ Step 9: Created result DataFrame in 7.9618 seconds.\n",
      "🚀 Total Execution Time: 67.2999 seconds.\n"
     ]
    }
   ],
   "source": [
    "# df_ik_pairs = create_i_k_pairs(df=df_raw, trajectories=filtered_df[['ChosenID','md','tvd','inclination','azimuth','latitude','longitude','x','y','z']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
