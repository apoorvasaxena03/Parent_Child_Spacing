{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas package for data manipulation and analysis\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None) # Set the maximum number of columns to display to None\n",
    "\n",
    "import numpy as np # Importing numpy package for numerical operations\n",
    "\n",
    "from typing import Dict, Tuple, List, Union, Optional # Importing specific types from typing module\n",
    "\n",
    "from src.database_manager import DatabricksOdbcConnector # Importing DatabricksOdbcConnector class from database_manager module\n",
    "\n",
    "import time # Importing Time Module\n",
    "\n",
    "import pyproj # Importing pyproj package\n",
    "\n",
    "from src.custom_logger import CustomLogger # Importing CustomLogger class from custom\n",
    "\n",
    "import os # Importing os module for operating system dependent functionality\n",
    "\n",
    "# Importing necessary modules for plotting and data manipulation\n",
    "import matplotlib.pyplot as plt # Importing matplotlib.pyplot for plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D # Importing 3D plotting toolkit from matplotlib\n",
    "\n",
    "# Setting matplotlib to inline mode for Jupyter notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg' # Configuring inline backend to use SVG format for figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WellDataLoader:\n",
    "    \"\"\"\n",
    "    A class to load well header and directional survey data either from file (CSV/Excel)\n",
    "    or directly from a database. It supports flexible column mapping for file-based inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db: Optional[object] = None,\n",
    "        log_dir: str = \"./logs\"\n",
    "    ):\n",
    "        self.db = db\n",
    "        self.logger = CustomLogger(\"well_data_loader\", \"WellDataLoaderLogger\", log_dir).get_logger()\n",
    "        self.header_df = pd.DataFrame()\n",
    "\n",
    "    def load_data_from_file(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        required_columns: Dict[str, str],\n",
    "        dtype: Optional[Dict[str, type]] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        try:\n",
    "            usecols = list(required_columns.values())\n",
    "            if file_path.endswith(('.xlsx', '.xls')):\n",
    "                df = pd.read_excel(file_path, dtype=dtype, usecols=usecols)\n",
    "            elif file_path.endswith('.csv'):\n",
    "                df = pd.read_csv(file_path, dtype=dtype, usecols=usecols)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file type. Use CSV or Excel.\")\n",
    "\n",
    "            missing = [val for val in required_columns.values() if val not in df.columns]\n",
    "            if missing:\n",
    "                raise ValueError(f\"Missing required columns in file: {missing}\")\n",
    "\n",
    "            return df.rename(columns={v: k for k, v in required_columns.items()})\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load data from file {file_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_header_data(\n",
    "        self,\n",
    "        source: Optional[Union[str, pd.DataFrame]] = None,\n",
    "        column_map: Optional[Dict[str, str]] = None,\n",
    "        basin: str = \"MB\",\n",
    "        start_year: int = 2019,\n",
    "        dtype: Optional[Dict[str, type]] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        if isinstance(source, pd.DataFrame):\n",
    "            self.logger.info(\"Using provided header DataFrame.\")\n",
    "            df = source\n",
    "        elif isinstance(source, str) and os.path.exists(source):\n",
    "            if not column_map:\n",
    "                raise ValueError(\"Column map must be provided when reading from file.\")\n",
    "            self.logger.info(f\"Loading header data from file: {source}\")\n",
    "            df = self.load_data_from_file(source, column_map, dtype=dtype)\n",
    "        elif source is None:\n",
    "            self.logger.info(\"Loading header data from SQL.\")\n",
    "            df = self._query_header_from_db(basin, start_year)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input: provide either a file path, DataFrame, or SQL query.\")\n",
    "\n",
    "        self.header_df = df\n",
    "        return df\n",
    "\n",
    "    def get_directional_data(\n",
    "        self,\n",
    "        source: Optional[str] = None,\n",
    "        column_map: Optional[Dict[str, str]] = None,\n",
    "        dtype: Optional[Dict[str, type]] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        if source and os.path.exists(source):\n",
    "            if not column_map:\n",
    "                raise ValueError(\"Column map must be provided when reading from file.\")\n",
    "            self.logger.info(f\"Loading directional data from file: {source}\")\n",
    "            return self.load_data_from_file(source, column_map, dtype=dtype)\n",
    "        elif source is None:\n",
    "            self.logger.info(\"Loading directional data from SQL.\")\n",
    "            return self._query_directional_from_db()\n",
    "        else:\n",
    "            raise ValueError(\"Provide either a file path or SQL query for directional data.\")\n",
    "\n",
    "    def _query_header_from_db(self, basin: str, start_year: int) -> pd.DataFrame:\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            api14 AS uwi, \n",
    "            leaseName AS lease_name,\n",
    "            wellName AS well_name,\n",
    "            wellNumber AS well_num,\n",
    "            currentOperator AS operator,\n",
    "            customString2 AS rsv_cat,\n",
    "            customString0 AS bench,\n",
    "            DATE(firstProdDate) AS first_prod_date,\n",
    "            holeDirection AS hole_direction,\n",
    "            surfaceLatitude AS surface_lat,\n",
    "            surfaceLongitude AS surface_lon\n",
    "        FROM Combocurve.export.wells\n",
    "        WHERE basin = '{basin}'\n",
    "          AND customString2 in (\"01PDP\", \"02PDNP\", \"02PA\") \n",
    "          AND holeDirection = 'H' \n",
    "          AND YEAR(DATE(firstProdDate)) >= {start_year}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.db.connect()\n",
    "            df = self.db.execute_query(query)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving header data from databricks: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.db.close_connection()\n",
    "\n",
    "    def _query_directional_from_db(self) -> pd.DataFrame:\n",
    "        if self.header_df.empty or 'uwi' not in self.header_df.columns:\n",
    "            raise ValueError(\"Header data must be loaded before querying directional data, and must contain a 'uwi' column.\")\n",
    "\n",
    "        uwis = \", \".join(f\"'{id}'\" for id in self.header_df['uwi'].unique())\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "            uwi, \n",
    "            station_md_uscust AS md, \n",
    "            station_tvd_uscust AS tvd,\n",
    "            inclination, \n",
    "            azimuth, \n",
    "            latitude, \n",
    "            longitude, \n",
    "            x_offset_uscust AS `deviation_E/W`,\n",
    "            ew_direction as `E/W`,\n",
    "            y_offset_uscust AS `deviation_N/S`,\n",
    "            ns_direction  as `N/S`,\n",
    "            point_type as point_type_name\n",
    "        FROM ihs_sp.well.well_directional_survey_station\n",
    "        WHERE uwi IN ({uwis})\n",
    "        ORDER BY uwi, md;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.db.connect()\n",
    "            df = self.db.execute_query(query)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving directional data from databricks: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            self.db.close_connection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoSurveyProcessor:\n",
    "    \"\"\"\n",
    "    A class for processing directional survey data and performing geospatial transformations\n",
    "    such as converting lat/lon to UTM, filtering heel points, and extracting key well locations.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_dir: str = \"./logs\",):\n",
    "        \"\"\"\n",
    "        Initializes the GeoSurveyProcessor with optional header data and log directory.\n",
    "        :param log_dir: Directory for logging.\n",
    "        \"\"\"\n",
    "        self.logger = CustomLogger(\"geo_processor\", \"GeoLogger\", log_dir).get_logger()  # Custom logger\n",
    "        self.logger.info(\"GeoSurveyProcessor initialized.\")\n",
    "\n",
    "    def determine_utm_zone(self, longitude: float) -> int:\n",
    "        \"\"\"\n",
    "        Determines the UTM zone based on a given longitude.\n",
    "        \"\"\"\n",
    "        return int((longitude + 180) / 6) + 1\n",
    "        \n",
    "    def convert_utm_to_latlon(self, \n",
    "                              df: pd.DataFrame, x_col: str = \"x\", y_col: str = \"y\", \n",
    "                              zone_col: str = \"utm_zone\", epsg_col: str = \"epsg_code\", \n",
    "                              lat_col: str = \"latitude\", lon_col: str = \"longitude\",\n",
    "                              round_output: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Converts UTM (x, y) coordinates back to lat/lon using EPSG codes or UTM zones.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame with UTM x/y in feet and a zone identifier.\n",
    "        - x_col, y_col: Column names for UTM coordinates (in feet).\n",
    "        - zone_col: Column containing UTM zone numbers.\n",
    "        - epsg_col: Optional EPSG code column (e.g., 'EPSG:32613'). If not present, it will be constructed from zone_col.\n",
    "        - round_output: Whether to round output lat/lon to 8 decimal places.\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame with added columns: 'lon_from_utm' and 'lat_from_utm'\n",
    "        \"\"\"\n",
    "\n",
    "        df = df.copy()\n",
    "        \n",
    "        if epsg_col not in df.columns:\n",
    "            df[epsg_col] = df[zone_col].apply(lambda z: f\"EPSG:326{int(z)}\")\n",
    "\n",
    "        df[lat_col] = np.nan\n",
    "        df[lon_col] = np.nan\n",
    "\n",
    "        for epsg in df[epsg_col].unique():\n",
    "            mask = df[epsg_col] == epsg\n",
    "\n",
    "            # Convert feet to meters\n",
    "            x_m = df.loc[mask, x_col] / 3.28084\n",
    "            y_m = df.loc[mask, y_col] / 3.28084\n",
    "\n",
    "            transformer = pyproj.Transformer.from_crs(epsg, \"EPSG:4326\", always_xy=True)\n",
    "            lon, lat = transformer.transform(x_m.values, y_m.values)\n",
    "\n",
    "            if round_output:\n",
    "                lon = np.round(lon, 8)\n",
    "                lat = np.round(lat, 8)\n",
    "\n",
    "            df.loc[mask, lat_col] = lat\n",
    "            df.loc[mask, lon_col] = lon\n",
    "\n",
    "        self.logger.info(f\"âœ… Back-converted UTM to lat/lon for {len(df)} rows.\")\n",
    "        return df\n",
    "    \n",
    "    def compute_utm_coordinates(self, df: pd.DataFrame, surface_df: Optional[pd.DataFrame] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Computes UTM (x, y, z) coordinates in feet. Uses per-row lat/lon if available.\n",
    "        Otherwise uses surface_df to compute UTM using deviation displacements.\n",
    "\n",
    "        Adds EPSG code used per row. Optionally back-computes lat/lon from UTM for verification.\n",
    "\n",
    "        Parameters:\n",
    "        - df: pd.DataFrame with directional survey data.\n",
    "        - surface_df: Optional pd.DataFrame with ['uwi', 'surface_lat', 'surface_lon'].\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame with UTM coordinates 'x', 'y', 'z', 'utm_zone', 'epsg_code', and optionally back-computed lat/lon.\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        df = df.sort_values(by=[\"uwi\", \"md\"]).copy()\n",
    "\n",
    "        df[\"x\"], df[\"y\"] = np.zeros(len(df)), np.zeros(len(df))\n",
    "\n",
    "        if \"latitude\" in df.columns and \"longitude\" in df.columns:\n",
    "            self.logger.info(\"âœ… Using lat/lon from input DataFrame.\")\n",
    "            df[\"utm_zone\"] = df[\"longitude\"].apply(self.determine_utm_zone)\n",
    "\n",
    "            for zone in df[\"utm_zone\"].unique():\n",
    "                epsg_code = f\"EPSG:326{zone}\"\n",
    "                mask = df[\"utm_zone\"] == zone\n",
    "\n",
    "                transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", epsg_code, always_xy=True)\n",
    "                lon = df.loc[mask, \"longitude\"].values\n",
    "                lat = df.loc[mask, \"latitude\"].values\n",
    "                easting_m, northing_m = transformer.transform(lon, lat)\n",
    "\n",
    "                df.loc[mask, \"x\"] = easting_m * 3.28084\n",
    "                df.loc[mask, \"y\"] = northing_m * 3.28084\n",
    "                df.loc[mask, \"epsg_code\"] = epsg_code\n",
    "\n",
    "        elif surface_df is not None:\n",
    "            self.logger.info(\"ðŸ§­ Lat/Lon not available â€” using surface_df and displacements.\")\n",
    "            \n",
    "            required_cols = {\"uwi\", \"surface_lat\", \"surface_lon\"}\n",
    "            \n",
    "            if not required_cols.issubset(surface_df.columns):\n",
    "                raise ValueError(f\"surface_df must contain {required_cols}\")\n",
    "\n",
    "            df = df.merge(surface_df, on=\"uwi\", how=\"left\")\n",
    "            df[\"utm_zone\"] = df[\"surface_lon\"].apply(self.determine_utm_zone)\n",
    "\n",
    "            for zone in df[\"utm_zone\"].unique():\n",
    "                epsg_code = f\"EPSG:326{zone}\"\n",
    "                mask = df[\"utm_zone\"] == zone\n",
    "\n",
    "                transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", epsg_code, always_xy=True)\n",
    "                lon = df.loc[mask, \"surface_lon\"].values\n",
    "                lat = df.loc[mask, \"surface_lat\"].values\n",
    "                easting_m, northing_m = transformer.transform(lon, lat)\n",
    "\n",
    "                # Convert to feet\n",
    "                easting_ft = easting_m * 3.28084\n",
    "                northing_ft = northing_m * 3.28084\n",
    "\n",
    "                ew_sign = df.loc[mask, \"E/W\"].map({\"E\": 1, \"W\": -1}).fillna(0)\n",
    "                ns_sign = df.loc[mask, \"N/S\"].map({\"N\": 1, \"S\": -1}).fillna(0)\n",
    "\n",
    "                df.loc[mask, \"x\"] = easting_ft + df.loc[mask, \"deviation_E/W\"] * ew_sign\n",
    "                df.loc[mask, \"y\"] = northing_ft + df.loc[mask, \"deviation_N/S\"] * ns_sign\n",
    "                df.loc[mask, \"epsg_code\"] = epsg_code\n",
    "\n",
    "            # Back-convert to lat/lon using x/y and epsg_code\n",
    "            df = self.convert_utm_to_latlon(df)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Either lat/lon must be present in df, or surface_df must be provided.\")\n",
    "\n",
    "        df[\"z\"] = -df[\"tvd\"]\n",
    "\n",
    "        self.logger.info(f\"âœ… UTM coordinate computation complete in {time.time() - start_time:.2f} sec.\")\n",
    "        return df\n",
    "    \n",
    "    def filter_after_heel_point(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filters the dataframe to include all rows for each uwi where the first occurrence \n",
    "        of either '80' or 'heel' appears in the point_type column and all subsequent rows.\n",
    "\n",
    "        Parameters:\n",
    "        df (pd.DataFrame): A dataframe containing directional survey data with a 'uwi' column and 'point_type' column.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: Filtered dataframe containing rows from the first occurrence of '80' or 'heel' onward.\n",
    "        \"\"\"\n",
    "        # Ensure the data is sorted by MD in ascending order\n",
    "        df = df.sort_values(by=[\"uwi\", \"md\"], ascending=True).copy()\n",
    "\n",
    "        # Convert 'point_type_name' to lowercase and check for '80' or 'heel'\n",
    "        mask = df['point_type_name'].str.lower().str.contains(r'80|heel', regex=True, na=False)\n",
    "\n",
    "        # Identify the first occurrence for each uwi\n",
    "        idx_start = df[mask].groupby('uwi', sort=False).head(1).index\n",
    "\n",
    "        # Create a mapping of uwi to the starting index\n",
    "        start_idx_map = dict(zip(df.loc[idx_start, 'uwi'], idx_start))\n",
    "\n",
    "        # Create a boolean mask using NumPy to filter rows\n",
    "        uwis = df['uwi'].values\n",
    "        indices = np.arange(len(df))\n",
    "\n",
    "        # Get the minimum start index for each row's uwi\n",
    "        start_indices = np.vectorize(start_idx_map.get, otypes=[float])(uwis)\n",
    "\n",
    "        # Mask rows where index is greater than or equal to the start index\n",
    "        valid_rows = indices >= start_indices\n",
    "\n",
    "        return df[valid_rows].reset_index(drop=True)\n",
    "    \n",
    "    def get_heel_toe_midpoints_latlon(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract the heel, toe, and mid-point latitude/longitude for each uwi in the well trajectory DataFrame\n",
    "        that has been filtered to have lateral section of the well.\n",
    "\n",
    "        Parameters:\n",
    "        df: pd.DataFrame\n",
    "            DataFrame containing well trajectory data, including 'uwi', 'md', 'latitude', and 'longitude'.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame\n",
    "            A DataFrame with 'uwi', 'Heel_Lat', 'Heel_Lon', 'Toe_Lat', 'Toe_Lon', 'Mid_Lat', 'Mid_Lon'.\n",
    "\n",
    "        Example:\n",
    "        >>> data = {\n",
    "        ...     \"uwi\": [1001, 1001, 1001, 1002, 1002],\n",
    "        ...     \"md\": [5000, 5100, 5200, 6000, 6100],\n",
    "        ...     \"latitude\": [31.388, 31.389, 31.387, 31.400, 31.401],\n",
    "        ...     \"longitude\": [-103.314, -103.315, -103.316, -103.318, -103.319]\n",
    "        ... }\n",
    "        >>> df = pd.DataFrame(data)\n",
    "        >>> extract_heel_toe_mid_lat_lon(df)\n",
    "        uwi  Heel_Lat  Heel_Lon  Toe_Lat  Toe_Lon  Mid_Lat  Mid_Lon\n",
    "        0     1001    31.388  -103.314   31.387  -103.316  31.3875 -103.315\n",
    "        1     1002    31.400  -103.318   31.401  -103.319  31.4005 -103.3185\n",
    "        \"\"\"\n",
    "        # Getting DataFrame with only the rows after the heel point\n",
    "        df = self.filter_after_heel_point(df)\n",
    "\n",
    "        # Group by 'uwi' and extract heel/toe lat/lon\n",
    "        heel_toe_df = (\n",
    "            df.groupby(\"uwi\")\n",
    "            .agg(\n",
    "                heel_lat=(\"latitude\", \"first\"),\n",
    "                heel_lon=(\"longitude\", \"first\"),\n",
    "                toe_lat=(\"latitude\", \"last\"),\n",
    "                toe_lon=(\"longitude\", \"last\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Calculate midpoints\n",
    "        heel_toe_df[\"mid_Lat\"] = (heel_toe_df[\"heel_lat\"] + heel_toe_df[\"toe_lat\"]) / 2\n",
    "        heel_toe_df[\"mid_Lon\"] = (heel_toe_df[\"heel_lon\"] + heel_toe_df[\"toe_lon\"]) / 2\n",
    "\n",
    "        return heel_toe_df\n",
    "    \n",
    "    def plot_utm_trajectory(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        plot_3d: bool = True,\n",
    "        uwis: Optional[Union[list, str]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Visualizes the UTM trajectory (2D or 3D) for one or multiple wells.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): DataFrame with 'x', 'y', 'z', and 'uwi' columns.\n",
    "        - plot_3d (bool): Whether to plot in 3D (True) or 2D (False). Defaults to True.\n",
    "        - uwis (Optional[list or str]): One or more specific uwis to filter and plot. Defaults to all wells.\n",
    "        \"\"\"\n",
    "        if uwis is not None:\n",
    "            if isinstance(uwis, str):\n",
    "                uwis = [uwis]\n",
    "            df = df[df[\"uwi\"].isin(uwis)]\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        title = \"3D Well Trajectory (UTM ft)\" if plot_3d else \"2D Well Plan View (x-y, UTM ft)\"\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "\n",
    "        if plot_3d:\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            for uwi, group in df.groupby(\"uwi\"):\n",
    "                ax.plot(group[\"x\"], group[\"y\"], group[\"z\"], label=str(uwi))\n",
    "            ax.set_zlabel(\"Z (ft, -TVD)\")\n",
    "        else:\n",
    "            ax = fig.add_subplot(111)\n",
    "            for uwi, group in df.groupby(\"uwi\"):\n",
    "                ax.plot(group[\"x\"], group[\"y\"], label=str(uwi))\n",
    "        \n",
    "        ax.set_xlabel(\"X (Easting, ft)\")\n",
    "        ax.set_ylabel(\"Y (Northing, ft)\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
