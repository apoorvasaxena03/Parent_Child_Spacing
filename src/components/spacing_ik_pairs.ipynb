{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Well Bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing / Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Importing pandas package\n",
    "\n",
    "# Set the maximum number of columns to display to None\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy as np # Importing numpy package\n",
    "\n",
    "from typing import Dict, Tuple, List, Union # Importing specific types from typing module\n",
    "\n",
    "import re # Importing regular expression package\n",
    "\n",
    "from src.database_manager import DatabricksOdbcConnector # Importing DatabricksOdbcConnector class from database_manager module\n",
    "from src.utils import reorder_columns # Importing reorder_columns function from utils module\n",
    "\n",
    "from scipy.spatial.distance import cdist # Importing cdist function from scipy package\n",
    "\n",
    "import time\n",
    "\n",
    "import pyproj # Importing pyproj package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Excel/csv into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('wellHeader_with_Cluster.csv',dtype={'ChosenID':str},parse_dates=['FirstProdDate','Comp_Dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming cluster column to bundle\n",
    "df_raw = df_raw.rename(columns={'cluster':'bundle'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by a specific column (e.g., 'bundle') in ascending order\n",
    "df_raw.sort_values(by='bundle', ascending=True, ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2514, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Creating DSU Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DSU columns names from Lease Name columns\n",
    "\n",
    "df_raw['DSU'] = df_raw['LeaseName'].apply(\n",
    "    lambda x: re.sub(r'[^a-zA-Z\\s]', ' ',  # Remove special characters, keep letters and spaces\n",
    "                     re.match(r'([^\\d]+)', str(x)).group(1) if pd.notna(x) and re.match(r'([^\\d]+)', str(x)) else str(x))  \n",
    "                    .strip()  # Strip leading/trailing spaces\n",
    ").replace(r'\\s+', ' ', regex=True)  # Collapse multiple spaces into a single space\n",
    "\n",
    "# Placing DSU next to LeaseName\n",
    "df_raw = reorder_columns(df=df_raw, columns_to_move=['DSU'], reference_column='LeaseName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Creating dataframes that have more than one unique bundles or DSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the same DSU has more than one unique bundle\n",
    "same_DSU_diffBundle_df = df_raw[df_raw.groupby(\"DSU\")[\"bundle\"].transform(\"nunique\") > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the same bundle has more than one unique DSU\n",
    "same_Bundle_diffDSU_df = df_raw[df_raw.groupby(\"bundle\")[\"DSU\"].transform(\"nunique\") > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Defining Functions that is used in calculation for i-k pair dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_heel_toe_mid_lat_lon(well_trajectory: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the heel, toe, and mid-point latitude/longitude for each ChosenID in the well trajectory DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    well_trajectory: pd.DataFrame\n",
    "        DataFrame containing well trajectory data, including 'ChosenID', 'md', 'latitude', and 'longitude'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        A DataFrame with 'ChosenID', 'Heel_Lat', 'Heel_Lon', 'Toe_Lat', 'Toe_Lon', 'Mid_Lat', 'Mid_Lon'.\n",
    "\n",
    "    Example:\n",
    "    >>> data = {\n",
    "    ...     \"ChosenID\": [1001, 1001, 1001, 1002, 1002],\n",
    "    ...     \"md\": [5000, 5100, 5200, 6000, 6100],\n",
    "    ...     \"latitude\": [31.388, 31.389, 31.387, 31.400, 31.401],\n",
    "    ...     \"longitude\": [-103.314, -103.315, -103.316, -103.318, -103.319]\n",
    "    ... }\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> extract_heel_toe_mid_lat_lon(df)\n",
    "       ChosenID  Heel_Lat  Heel_Lon  Toe_Lat  Toe_Lon  Mid_Lat  Mid_Lon\n",
    "    0     1001    31.388  -103.314   31.387  -103.316  31.3875 -103.315\n",
    "    1     1002    31.400  -103.318   31.401  -103.319  31.4005 -103.3185\n",
    "    \"\"\"\n",
    "    # Ensure the data is sorted by MD in ascending order\n",
    "    well_trajectory = well_trajectory.sort_values(by=[\"ChosenID\", \"md\"], ascending=True)\n",
    "\n",
    "    # Group by 'ChosenID' and extract heel/toe lat/lon\n",
    "    heel_toe_df = (\n",
    "        well_trajectory.groupby(\"ChosenID\")\n",
    "        .agg(\n",
    "            heel_lat=(\"latitude\", \"first\"),\n",
    "            heel_lon=(\"longitude\", \"first\"),\n",
    "            toe_lat=(\"latitude\", \"last\"),\n",
    "            toe_lon=(\"longitude\", \"last\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calculate midpoints\n",
    "    heel_toe_df[\"mid_Lat\"] = (heel_toe_df[\"heel_lat\"] + heel_toe_df[\"toe_lat\"]) / 2\n",
    "    heel_toe_df[\"mid_Lon\"] = (heel_toe_df[\"heel_lon\"] + heel_toe_df[\"toe_lon\"]) / 2\n",
    "\n",
    "    return heel_toe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(lat1: np.ndarray, lon1: np.ndarray, lat2: np.ndarray, lon2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Determine the relative direction of (lat2, lon2) with respect to (lat1, lon1).\n",
    "    \n",
    "    Parameters:\n",
    "    lat1, lon1: np.ndarray\n",
    "        Latitude and longitude of the first well.\n",
    "    lat2, lon2: np.ndarray\n",
    "        Latitude and longitude of the second well.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray\n",
    "        Array indicating the direction (e.g., North, South, East, West) of well B relative to well A.\n",
    "    \"\"\"\n",
    "    lat_diff = lat2 - lat1\n",
    "    lon_diff = lon2 - lon1\n",
    "\n",
    "    conditions = [\n",
    "        np.abs(lat_diff) > np.abs(lon_diff),\n",
    "        lat_diff > 0,\n",
    "        lon_diff > 0\n",
    "    ]\n",
    "\n",
    "    choices = [\"North\", \"South\", \"East\", \"West\"]\n",
    "    \n",
    "    return np.select(\n",
    "        [conditions[0] & conditions[1], conditions[0] & ~conditions[1], ~conditions[0] & conditions[2], ~conditions[0] & ~conditions[2]],\n",
    "        choices\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_drill_direction_vectorized(well_trajectories: Dict[str, pd.DataFrame], i_indices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized function to determine the drilling direction of multiple wells using NumPy operations.\n",
    "    \n",
    "    Parameters:\n",
    "    well_trajectories: Dict[str, pd.DataFrame]\n",
    "        Dictionary containing well trajectory data indexed by ChosenID.\n",
    "    i_indices: np.ndarray\n",
    "        Array of ChosenID whose drill directions need to be calculated.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray\n",
    "        Array containing \"EW\" (East-West) or \"NS\" (North-South) for each well.\n",
    "    \"\"\"\n",
    "    azimuth_values = np.array([well_trajectories[i][\"azimuth\"].mean() if not well_trajectories[i].empty else np.nan for i in i_indices])\n",
    "    \n",
    "    conditions = (45 <= azimuth_values) & (azimuth_values < 135) | (225 <= azimuth_values) & (azimuth_values < 315)\n",
    "    drill_directions = np.where(np.isnan(azimuth_values), \"Unknown\", np.where(conditions, \"EW\", \"NS\"))\n",
    "    \n",
    "    return drill_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_calculate_3D_distance_matrix(\n",
    "    trajectories: Dict[str, pd.DataFrame], i_indices: np.ndarray, k_indices: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Fully vectorized 3D distance calculations for well pairs using NumPy and Pandas.\n",
    "    \n",
    "    Parameters:\n",
    "    trajectories: Dict[str, pd.DataFrame]\n",
    "        Dictionary containing well trajectory data indexed by well ID.\n",
    "    i_indices: np.ndarray\n",
    "        Array of well IDs representing the first well in each pair.\n",
    "    k_indices: np.ndarray\n",
    "        Array of well IDs representing the second well in each pair.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        - Horizontal distances between the well pairs.\n",
    "        - Vertical distances between the well pairs.\n",
    "        - 3D distances between the well pairs.\n",
    "    \"\"\"\n",
    "    # ðŸš€ Precompute mean (midpoint) for each well ID across all wells at once\n",
    "    all_trajectories_df = pd.concat(trajectories.values(), keys=trajectories.keys()).reset_index(drop=True)\n",
    "\n",
    "    midpoints_df = all_trajectories_df.groupby(\"ChosenID\")[[\"x\", \"y\", \"tvd\"]].mean()\n",
    "\n",
    "    # Convert to NumPy arrays for fast lookup\n",
    "    well_ids = midpoints_df.index.to_numpy()\n",
    "    midpoints = midpoints_df.to_numpy()\n",
    "\n",
    "    # Create a mapping from well ID to its index\n",
    "    well_id_to_idx = {well_id: idx for idx, well_id in enumerate(well_ids)}\n",
    "\n",
    "    # Efficiently extract midpoints using NumPy indexing\n",
    "    mid_A = midpoints[np.array([well_id_to_idx[i] for i in i_indices])]\n",
    "    mid_B = midpoints[np.array([well_id_to_idx[k] for k in k_indices])]\n",
    "\n",
    "    # Compute distances\n",
    "    vertical_distances = np.abs(mid_A[:, 2] - mid_B[:, 2])\n",
    "    mid_B[:, 2] = mid_A[:, 2]  # Align Well B to Well Aâ€™s TVD\n",
    "\n",
    "    horizontal_distances = np.linalg.norm(mid_A[:, :2] - mid_B[:, :2], axis=1)\n",
    "    total_3D_distances = np.sqrt(horizontal_distances**2 + vertical_distances**2)\n",
    "\n",
    "    return horizontal_distances, vertical_distances, total_3D_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_i_k_pairs(df: pd.DataFrame, trajectories: Union[Dict[str, pd.DataFrame], pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate the i_k_pairs DataFrame, computing horizontal and vertical distances, \n",
    "    3D distances, drilling directions, and relative directions between well pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    df: pd.DataFrame\n",
    "        DataFrame containing well metadata with:\n",
    "        - \"ChosenID\" (str): Unique well identifier.\n",
    "\n",
    "    trajectories: Union[Dict[str, pd.DataFrame], pd.DataFrame]\n",
    "        Either:\n",
    "        - A dictionary mapping well IDs (\"ChosenID\") to trajectory DataFrames.\n",
    "        - A single DataFrame containing all trajectory data (must have \"ChosenID\" column).\n",
    "        \n",
    "    Each trajectory DataFrame should include:\n",
    "    - \"md\" (float): Measured depth.\n",
    "    - \"tvd\" (float): True vertical depth.\n",
    "    - \"inclination\" (float): Inclination angle in degrees.\n",
    "    - \"azimuth\" (float): represents the drilling direction.\n",
    "    - \"latitude\" (float): Latitude values, define the geographical position.\n",
    "    - \"longitude\" (float): Longitude values, define the geographical position.\n",
    "    - \"x\" (float): X-coordinate in a Cartesian coordinate system.\n",
    "    - \"y\" (float): Y-coordinate in a Cartesian coordinate system.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        DataFrame containing pairs of wells (`i_uwi`, `k_uwi`) with their computed distances \n",
    "        and directional relationships.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to dictionary if input is a DataFrame\n",
    "    step1_start = time.time()\n",
    "    if isinstance(trajectories, pd.DataFrame):\n",
    "        if \"ChosenID\" not in trajectories.columns:\n",
    "            raise ValueError(\"ðŸš¨ Error: Trajectory DataFrame must contain a 'ChosenID' column.\")\n",
    "        trajectories = {cid: group for cid, group in trajectories.groupby(\"ChosenID\")}\n",
    "    step1_end = time.time()\n",
    "    print(f\"âœ… Step 1: Converted trajectory DataFrame to dictionary in {step1_end - step1_start:.4f} seconds.\")\n",
    "\n",
    "    # Get unique ChosenIDs from df\n",
    "    step2_start = time.time()\n",
    "    chosen_ids = df[\"ChosenID\"].unique()\n",
    "    missing_ids = [cid for cid in chosen_ids if cid not in trajectories]\n",
    "\n",
    "    if missing_ids:\n",
    "        print(f\"âš ï¸ The following ChosenIDs do not exist in the trajectory data and will be excluded: {missing_ids}\")\n",
    "\n",
    "    df = df[df[\"ChosenID\"].isin(trajectories)] # Filter out missing IDs in the DataFrame\n",
    "    chosen_ids = df[\"ChosenID\"].unique() # Update chosen_ids without missing IDs\n",
    "    step2_end = time.time()\n",
    "    print(f\"âœ… Step 2: Extracted unique ChosenIDs in {step2_end - step2_start:.4f} seconds.\")\n",
    "\n",
    "    # Generate all possible pairs (excluding self-comparison)\n",
    "    step3_start = time.time()\n",
    "    i_uwi, k_uwi = np.meshgrid(chosen_ids, chosen_ids, indexing='ij')\n",
    "    i_uwi, k_uwi = i_uwi.ravel(), k_uwi.ravel()\n",
    "\n",
    "    # Remove self-comparisons\n",
    "    valid_mask = i_uwi != k_uwi\n",
    "    i_uwi, k_uwi = i_uwi[valid_mask], k_uwi[valid_mask]\n",
    "    step3_end = time.time()\n",
    "    print(f\"âœ… Step 3: Generated well pairs in {step3_end - step3_start:.4f} seconds.\")\n",
    "\n",
    "    # ðŸš€ Optimized Heel/Toe Extraction (Vectorized)\n",
    "    step4_start = time.time()\n",
    "    heel_toe_df = pd.concat(\n",
    "        [extract_heel_toe_mid_lat_lon(trajectories[cid]) for cid in chosen_ids], ignore_index=True\n",
    "    )\n",
    "    heel_toe_dict = heel_toe_df.set_index(\"ChosenID\").to_dict(orient=\"index\")\n",
    "    step4_end = time.time()\n",
    "    print(f\"âœ… Step 4: Heel/Toe extraction took {step4_end - step4_start:.4f} seconds.\")\n",
    "\n",
    "    # Efficiently extract values using vectorized lookups\n",
    "    step5_start = time.time()\n",
    "    heel_lat_i = np.array([heel_toe_dict[i][\"heel_lat\"] for i in i_uwi])\n",
    "    heel_lon_i = np.array([heel_toe_dict[i][\"heel_lon\"] for i in i_uwi])\n",
    "    toe_lat_k = np.array([heel_toe_dict[k][\"toe_lat\"] for k in k_uwi])\n",
    "    toe_lon_k = np.array([heel_toe_dict[k][\"toe_lon\"] for k in k_uwi])\n",
    "    step5_end = time.time()\n",
    "    print(f\"âœ… Step 5: Heel/Toe dictionary lookup took {step5_end - step5_start:.4f} seconds.\")\n",
    "\n",
    "    # ðŸš€ Optimized Distance Calculation (Fully Vectorized)\n",
    "    step6_start = time.time()\n",
    "    horizontal_dist, vertical_dist, total_3D_dist = optimized_calculate_3D_distance_matrix(trajectories, i_uwi, k_uwi)\n",
    "    step6_end = time.time()\n",
    "    print(f\"âœ… Step 6: Distance calculations took {step6_end - step6_start:.4f} seconds.\")\n",
    "\n",
    "    # Compute drill directions\n",
    "    step7_start = time.time()\n",
    "    drill_directions = calculate_drill_direction_vectorized(trajectories, i_uwi)\n",
    "    step7_end = time.time()\n",
    "    print(f\"âœ… Step 7: Drill direction calculation took {step7_end - step7_start:.4f} seconds.\")\n",
    "\n",
    "    # Determine directional relationship\n",
    "    step8_start = time.time()\n",
    "    ward_of_i = get_direction(heel_lat_i, heel_lon_i, toe_lat_k, toe_lon_k)\n",
    "    step8_end = time.time()\n",
    "    print(f\"âœ… Step 8: Directional relationship calculation took {step8_end - step8_start:.4f} seconds.\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    step9_start = time.time()\n",
    "    result_df = pd.DataFrame({\n",
    "        \"i_uwi\": i_uwi,\n",
    "        \"k_uwi\": k_uwi,\n",
    "        \"horizontal_dist\": horizontal_dist,\n",
    "        \"vertical_dist\": vertical_dist,\n",
    "        \"3D_ft_to_same\": total_3D_dist,\n",
    "        \"drill_direction\": drill_directions,\n",
    "        \"ward_of_i\": ward_of_i\n",
    "    })\n",
    "    step9_end = time.time()\n",
    "    print(f\"âœ… Step 9: Created result DataFrame in {step9_end - step9_start:.4f} seconds.\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"ðŸš€ Total Execution Time: {total_time:.4f} seconds.\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(well_A: pd.DataFrame, well_B: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the percentage overlap between two horizontal wellbores.\n",
    "    \n",
    "    Parameters:\n",
    "    well_A: pd.DataFrame\n",
    "        Well trajectory data for Well A, including 'MD' (Measured Depth) and 'Inclination'.\n",
    "    well_B: pd.DataFrame\n",
    "        Well trajectory data for Well B, including 'MD' (Measured Depth) and 'Inclination'.\n",
    "    \n",
    "    Returns:\n",
    "    float:\n",
    "        Percentage of overlap relative to the shorter lateral.\n",
    "    \"\"\"\n",
    "    if well_A.empty or well_B.empty:\n",
    "        return 0.0\n",
    "\n",
    "    start_A, end_A = well_A[\"MD\"].min(), well_A[\"MD\"].max()\n",
    "    start_B, end_B = well_B[\"MD\"].min(), well_B[\"MD\"].max()\n",
    "\n",
    "    overlap_start = max(start_A, start_B)\n",
    "    overlap_end = min(end_A, end_B)\n",
    "\n",
    "    if overlap_start >= overlap_end:\n",
    "        return 0.0\n",
    "\n",
    "    overlap_length = overlap_end - overlap_start\n",
    "    shorter_length = min(end_A - start_A, end_B - start_B)\n",
    "\n",
    "    return (overlap_length / shorter_length) * 100 if shorter_length > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Defining Functions that is used to compute Lat/Lon to UTM Co-Ordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_utm_zone(longitude: float) -> int:\n",
    "    \"\"\"\n",
    "    Determines the UTM zone based on a given longitude.\n",
    "    \"\"\"\n",
    "    return int((longitude + 180) / 6) + 1\n",
    "\n",
    "\n",
    "def batch_latlon_to_utm(lat: np.ndarray, lon: np.ndarray, utm_zone: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts arrays of latitudes and longitudes to UTM coordinates in meters for a given UTM zone.\n",
    "    \"\"\"\n",
    "    proj_utm = pyproj.Transformer.from_crs(\n",
    "        \"EPSG:4326\", f\"EPSG:326{utm_zone}\", always_xy=True\n",
    "    )\n",
    "    \n",
    "    return proj_utm.transform(lon, lat)\n",
    "\n",
    "\n",
    "def compute_utm_coordinates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes UTM (x, y, z) coordinates for multiple wells, using surface location to determine UTM zones.\n",
    "    Converts UTM coordinates from meters to feet. Uses vectorized batch processing for performance.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Original directional survey DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with all original columns + x, y, z (in feet), and utm_zone.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Step 1: Sort dataframe by md to identify surface location\n",
    "    df = df.sort_values(by=[\"ChosenID\", \"md\"], ascending=[True, True])\n",
    "    \n",
    "    # Step 2: Determine UTM zones using the surface location (first row per well)\n",
    "    surface_locs = df.groupby(\"ChosenID\").first()[[\"latitude\", \"longitude\"]]\n",
    "    surface_locs[\"utm_zone\"] = surface_locs[\"longitude\"].apply(determine_utm_zone)\n",
    "\n",
    "    # Merge UTM zones back into the original dataframe\n",
    "    df = df.merge(surface_locs[[\"utm_zone\"]], on=\"ChosenID\", how=\"left\")\n",
    "\n",
    "    print(f\"âœ… Determined UTM zones in {time.time() - start_time:.4f} seconds.\")\n",
    "\n",
    "    # Step 3: Batch transformation for each unique UTM zone\n",
    "    start_transform_time = time.time()\n",
    "    unique_zones = df[\"utm_zone\"].unique()\n",
    "    utm_converters: Dict[int, Tuple[np.ndarray, np.ndarray]] = {}\n",
    "\n",
    "    for zone in unique_zones:\n",
    "        subset = df[df[\"utm_zone\"] == zone]\n",
    "        easting, northing = batch_latlon_to_utm(subset[\"latitude\"].values, subset[\"longitude\"].values, zone)\n",
    "        utm_converters[zone] = (easting, northing)\n",
    "\n",
    "    print(f\"âœ… Performed batch EPSG transformations in {time.time() - start_transform_time:.4f} seconds.\")\n",
    "\n",
    "    # Step 4: Assign the converted coordinates back to the DataFrame\n",
    "    start_assign_time = time.time()\n",
    "    df[\"x\"], df[\"y\"] = np.zeros(len(df)), np.zeros(len(df))\n",
    "\n",
    "    for zone in unique_zones:\n",
    "        mask = df[\"utm_zone\"] == zone\n",
    "        df.loc[mask, \"x\"], df.loc[mask, \"y\"] = utm_converters[zone]\n",
    "\n",
    "    print(f\"âœ… Assigned transformed coordinates in {time.time() - start_assign_time:.4f} seconds.\")\n",
    "\n",
    "    # Step 5: Convert UTM coordinates from meters to feet (Conversion factor: 1 meter = 3.28084 feet)\n",
    "    df[\"x\"] *= 3.28084\n",
    "    df[\"y\"] *= 3.28084\n",
    "    \n",
    "    df[\"z\"] = -df[\"tvd\"] # Elevation is negative TVD\n",
    "\n",
    "    print(f\"âœ… Total execution time: {time.time() - start_time:.4f} seconds.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_after_heel_point(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters the dataframe to include all rows for each ChosenID where the first occurrence \n",
    "    of either '80' or 'heel' appears in the point_type column and all subsequent rows.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): A dataframe containing directional survey data with a 'ChosenID' column and 'point_type' column.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered dataframe containing rows from the first occurrence of '80' or 'heel' onward.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert 'point_type' to lowercase and check for '80' or 'heel'\n",
    "    mask = df['point_type'].str.lower().str.contains(r'80|heel', regex=True, na=False)\n",
    "\n",
    "    # Identify the first occurrence for each ChosenID\n",
    "    idx_start = df[mask].groupby('ChosenID', sort=False).head(1).index\n",
    "\n",
    "    # Create a mapping of ChosenID to the starting index\n",
    "    start_idx_map = dict(zip(df.loc[idx_start, 'ChosenID'], idx_start))\n",
    "\n",
    "    # Create a boolean mask using NumPy to filter rows\n",
    "    chosen_ids = df['ChosenID'].values\n",
    "    indices = np.arange(len(df))\n",
    "\n",
    "    # Get the minimum start index for each row's ChosenID\n",
    "    start_indices = np.vectorize(start_idx_map.get, otypes=[float])(chosen_ids)\n",
    "\n",
    "    # Mask rows where index is greater than or equal to the start index\n",
    "    valid_rows = indices >= start_indices\n",
    "\n",
    "    return df[valid_rows].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testinig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\apoorva.saxena\\onedrive - sitio royalties\\desktop\\project - apoorva\\python\\parent_child_spacing\\src\\database_manager.py:85: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result_df = pd.read_sql(sql_query, self.connection)\n"
     ]
    }
   ],
   "source": [
    "# Importing Directional Survey data from Databricks\n",
    "\n",
    "databricks = DatabricksOdbcConnector()\n",
    "\n",
    "# Filtering only Horizontal wells and getting their apis\n",
    "chosen_ids = \", \".join(f\"'{id}'\" for id in df_raw[df_raw['HoleDirection']=='H']['ChosenID'].unique())\n",
    "\n",
    "try:\n",
    "    databricks.connect()\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        LEFT(uwi, 10) AS ChosenID, \n",
    "        station_md_uscust AS md, \n",
    "        station_tvd_uscust AS tvd,\n",
    "        inclination, \n",
    "        azimuth, \n",
    "        latitude, \n",
    "        longitude, \n",
    "        x_offset_uscust AS `deviation_E/W`,\n",
    "        ew_direction,\n",
    "        y_offset_uscust AS `deviation_N/S`,\n",
    "        ns_direction,\n",
    "        point_type\n",
    "        \n",
    "    FROM ihs_sp.well.well_directional_survey_station\n",
    "    WHERE LEFT(uwi, 10) IN ({chosen_ids})\n",
    "    order by uwi, md;\n",
    "    \"\"\"\n",
    "\n",
    "    df_directional = databricks.execute_query(query)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    databricks.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Determined UTM zones in 0.7492 seconds.\n",
      "âœ… Performed batch EPSG transformations in 0.2651 seconds.\n",
      "âœ… Assigned transformed coordinates in 0.0107 seconds.\n",
      "âœ… Total execution time: 1.0466 seconds.\n"
     ]
    }
   ],
   "source": [
    "df_with_utm = compute_utm_coordinates(df_directional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filter_after_heel_point(df_with_utm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 1: Converted trajectory DataFrame to dictionary in 0.1134 seconds.\n",
      "âš ï¸ The following ChosenIDs do not exist in the trajectory data and will be excluded: ['4238935783', '4238933169', '4238940091', '4238937586', '4238930501', '4238932194', '4238937713', '4238933558', '4238930262', '4238933989', '4238931100', '4238937707', '4238910420', '4238939527', '4238940159', '4238940321', '4238940597', '4238940596', '4238940594', '4238940625']\n",
      "âœ… Step 2: Extracted unique ChosenIDs in 0.0125 seconds.\n",
      "âœ… Step 3: Generated well pairs in 0.2301 seconds.\n",
      "âœ… Step 4: Heel/Toe extraction took 16.6874 seconds.\n",
      "âœ… Step 5: Heel/Toe dictionary lookup took 4.1905 seconds.\n",
      "âœ… Step 6: Distance calculations took 2.5170 seconds.\n",
      "âœ… Step 7: Drill direction calculation took 145.5827 seconds.\n",
      "âœ… Step 8: Directional relationship calculation took 0.2239 seconds.\n",
      "âœ… Step 9: Created result DataFrame in 1.9453 seconds.\n",
      "ðŸš€ Total Execution Time: 171.5041 seconds.\n"
     ]
    }
   ],
   "source": [
    "df_ik_pairs = create_i_k_pairs(df=df_raw, trajectories=filtered_df[['ChosenID','md','tvd','inclination','azimuth','latitude','longitude','x','y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_two_values(ik_pair: pd.DataFrame, header: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the first two k_uwi values and corresponding horizontal_dist values\n",
    "    for each unique i_uwi. It processes the data separately based on:\n",
    "    \n",
    "    - When `Landing_Zone_i == Landing_Zone_k` (same landing zone)\n",
    "    - When `Landing_Zone_i != Landing_Zone_k` (different landing zones)\n",
    "    \n",
    "    The function sorts the data by `i_uwi` and `horizontal_dist` before grouping,\n",
    "    ensuring that the two nearest `k_uwi` values are extracted.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        A DataFrame containing the following columns:\n",
    "        - `i_uwi` (int): Unique identifier for well i\n",
    "        - `k_uwi` (int): Unique identifier for well k\n",
    "        - `Landing_Zone_i` (str): Landing zone for well i\n",
    "        - `Landing_Zone_k` (str): Landing zone for well k\n",
    "        - `horizontal_dist` (float): Distance between wells i and k\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing:\n",
    "        - `i_uwi`: Unique well identifier\n",
    "        - `k_uwi_same1`, `horizontal_dist_same1`: First closest match for the same landing zone\n",
    "        - `k_uwi_same2`, `horizontal_dist_same2`: Second closest match for the same landing zone\n",
    "        - `k_uwi_diff1`, `horizontal_dist_diff1`: First closest match for a different landing zone\n",
    "        - `k_uwi_diff2`, `horizontal_dist_diff2`: Second closest match for a different landing zone\n",
    "\n",
    "    Example Usage:\n",
    "    --------------\n",
    "    >>> data = {\n",
    "    ...     \"i_uwi\": [4238910251, 4238910251, 4238910251, 4238910251],\n",
    "    ...     \"k_uwi\": [4238932199, 4238934804, 4238910422, 4238932210],\n",
    "    ...     \"Landing_Zone_i\": [\"DEVONIAN\", \"DEVONIAN\", \"DEVONIAN\", \"DEVONIAN\"],\n",
    "    ...     \"Landing_Zone_k\": [\"DEVONIAN\", \"WCA\", \"DEVONIAN\", \"DEVONIAN\"],\n",
    "    ...     \"horizontal_dist\": [3293.97, 6238.26, 7789.78, 7950.35]\n",
    "    ... }\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> result = extract_top_two_values(df)\n",
    "    >>> print(result)\n",
    "    \"\"\"\n",
    "    \n",
    "    df_merge_ikPair_header = ik_pair.merge(header.rename(columns={'ChosenID':'i_uwi'})[['i_uwi','Landing_Zone']], how='left').merge(\n",
    "        header.rename(columns={'ChosenID':'k_uwi'})[['k_uwi','Landing_Zone']], how='left', on='k_uwi', suffixes=('_i', '_k'))\n",
    "    \n",
    "    df = df_merge_ikPair_header.groupby(['i_uwi','k_uwi','Landing_Zone_i','Landing_Zone_k'])[['horizontal_dist']].min().reset_index().sort_values(by=['i_uwi','horizontal_dist'], ascending=[True,True],ignore_index=True)\n",
    "    \n",
    "    # Sort DataFrame for stable ordering\n",
    "    df_sorted = df.sort_values(by=[\"i_uwi\", \"horizontal_dist\"])\n",
    "    \n",
    "    # Separate data into same and different Landing Zone groups\n",
    "    df_same = df_sorted[df_sorted[\"Landing_Zone_i\"] == df_sorted[\"Landing_Zone_k\"]]\n",
    "    df_diff = df_sorted[df_sorted[\"Landing_Zone_i\"] != df_sorted[\"Landing_Zone_k\"]]\n",
    "\n",
    "    # Get unique i_uwi values\n",
    "    unique_i_uwi = df_sorted[\"i_uwi\"].unique()\n",
    "\n",
    "    # Initialize arrays for both cases\n",
    "    k_uwi_same1 = np.full_like(unique_i_uwi, \"\", dtype=object)\n",
    "    horizontal_dist_same1 = np.full_like(unique_i_uwi, np.nan, dtype=np.float64)\n",
    "    k_uwi_same2 = np.full_like(unique_i_uwi, \"\", dtype=object)\n",
    "    horizontal_dist_same2 = np.full_like(unique_i_uwi, np.nan, dtype=np.float64)\n",
    "\n",
    "    k_uwi_diff1 = np.full_like(unique_i_uwi, \"\", dtype=object)\n",
    "    horizontal_dist_diff1 = np.full_like(unique_i_uwi, np.nan, dtype=np.float64)\n",
    "    k_uwi_diff2 = np.full_like(unique_i_uwi, \"\", dtype=object)\n",
    "    horizontal_dist_diff2 = np.full_like(unique_i_uwi, np.nan, dtype=np.float64)\n",
    "\n",
    "    # Group by 'i_uwi' and extract first two values\n",
    "    grouped_same = df_same.groupby(\"i_uwi\")[[\"k_uwi\", \"horizontal_dist\"]].apply(lambda x: x.values[:2])\n",
    "    grouped_diff = df_diff.groupby(\"i_uwi\")[[\"k_uwi\", \"horizontal_dist\"]].apply(lambda x: x.values[:2])\n",
    "\n",
    "    for idx, i_uwi in enumerate(unique_i_uwi):\n",
    "        values_same = grouped_same.get(i_uwi, [])\n",
    "        values_diff = grouped_diff.get(i_uwi, [])\n",
    "        \n",
    "        if len(values_same) > 0:\n",
    "            k_uwi_same1[idx], horizontal_dist_same1[idx] = str(values_same[0][0]), values_same[0][1]\n",
    "        if len(values_same) > 1:\n",
    "            k_uwi_same2[idx], horizontal_dist_same2[idx] = str(values_same[1][0]), values_same[1][1]\n",
    "        \n",
    "        if len(values_diff) > 0:\n",
    "            k_uwi_diff1[idx], horizontal_dist_diff1[idx] = str(values_diff[0][0]), values_diff[0][1]\n",
    "        if len(values_diff) > 1:\n",
    "            k_uwi_diff2[idx], horizontal_dist_diff2[idx] = str(values_diff[1][0]), values_diff[1][1]\n",
    "\n",
    "    # Create final DataFrame\n",
    "    return pd.DataFrame({\n",
    "        \"i_uwi\": unique_i_uwi,\n",
    "        \"k_uwi_same1\": k_uwi_same1.astype(object),\n",
    "        \"horizontal_dist_same1\": horizontal_dist_same1,\n",
    "        \"k_uwi_same2\": k_uwi_same2.astype(object),\n",
    "        \"horizontal_dist_same2\": horizontal_dist_same2,\n",
    "        \"k_uwi_near1\": k_uwi_diff1.astype(object),\n",
    "        \"horizontal_dist_near1\": horizontal_dist_diff1,\n",
    "        \"k_uwi_near2\": k_uwi_diff2.astype(object),\n",
    "        \"horizontal_dist_near2\": horizontal_dist_diff2\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_two_values_(ik_pair: pd.DataFrame, header: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts the first two `k_uwi` values and corresponding `horizontal_dist` values\n",
    "    for each unique `i_uwi` for:\n",
    "    - Same Landing Zone (`Landing_Zone_i == Landing_Zone_k`).\n",
    "    - Different Landing Zones (`Landing_Zone_i != Landing_Zone_k`).\n",
    "\n",
    "    Efficiently merges `WellName`, `DSU`, `RES_CAT`, `Landing_Zone`, `FirstProdDate` \n",
    "    for `i_uwi`, `k_uwi_same1`, `k_uwi_same2`, `k_uwi_near1`, and `k_uwi_near2`.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    ik_pair : pd.DataFrame\n",
    "        Contains `i_uwi`, `k_uwi`, and `horizontal_dist`.\n",
    "\n",
    "    header : pd.DataFrame\n",
    "        Contains well metadata including `WellName`, `DSU`, `RES_CAT`, `Landing_Zone`, `FirstProdDate`.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Enriched dataset containing closest wells with attributes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort header for efficient merging\n",
    "    header_sorted = header.sort_values(by=\"ChosenID\")\n",
    "\n",
    "    # Merge Landing_Zone information only once\n",
    "    df_merge = ik_pair.merge(\n",
    "        header_sorted[[\"ChosenID\", \"Landing_Zone\"]].rename(columns={'ChosenID': 'i_uwi'}),\n",
    "        how=\"left\", on=\"i_uwi\"\n",
    "    ).merge(\n",
    "        header_sorted[[\"ChosenID\", \"Landing_Zone\"]].rename(columns={'ChosenID': 'k_uwi'}),\n",
    "        how=\"left\", on=\"k_uwi\", suffixes=(\"_i\", \"_k\")\n",
    "    )\n",
    "\n",
    "    # Keep only the smallest `horizontal_dist` per (i_uwi, k_uwi)\n",
    "    df = df_merge.groupby(['i_uwi', 'k_uwi', 'Landing_Zone_i', 'Landing_Zone_k'])[['horizontal_dist']].min().reset_index()\n",
    "\n",
    "    # Sort DataFrame efficiently\n",
    "    df_sorted = df.sort_values(by=['i_uwi', 'horizontal_dist']).reset_index(drop=True)\n",
    "\n",
    "    # Extract top 2 `k_uwi` values separately for same & different landing zones\n",
    "    df_same = df_sorted[df_sorted[\"Landing_Zone_i\"] == df_sorted[\"Landing_Zone_k\"]].groupby(\"i_uwi\").head(2)\n",
    "    df_diff = df_sorted[df_sorted[\"Landing_Zone_i\"] != df_sorted[\"Landing_Zone_k\"]].groupby(\"i_uwi\").head(2)\n",
    "\n",
    "    def pivot_top_two(df, zone_type):\n",
    "        \"\"\"Helper function to pivot grouped data into `k_uwi_*` and `horizontal_dist_*` columns.\"\"\"\n",
    "        df[\"rank\"] = df.groupby(\"i_uwi\").cumcount() + 1  # Create ranking (1 or 2) for each i_uwi\n",
    "        df_pivot = df.pivot(index=\"i_uwi\", columns=\"rank\", values=[\"k_uwi\", \"horizontal_dist\"])\n",
    "\n",
    "        # Correct column renaming based on the landing zone type\n",
    "        df_pivot.columns = [\n",
    "            f\"k_uwi_{zone_type}{col[1]}\" if col[0] == \"k_uwi\" else f\"horizontal_dist_{zone_type}{col[1]}\"\n",
    "            for col in df_pivot.columns\n",
    "        ]\n",
    "\n",
    "        return df_pivot.reset_index()\n",
    "\n",
    "    # Pivot for both same and different Landing Zones\n",
    "    df_same_pivot = pivot_top_two(df_same, \"same\")\n",
    "    df_diff_pivot = pivot_top_two(df_diff, \"near\")\n",
    "\n",
    "    final_df = df_same_pivot.merge(df_diff_pivot, how=\"outer\", on=\"i_uwi\")\n",
    "    final_df = final_df.merge(header_sorted[['ChosenID','WellName','DSU','RES_CAT','Landing_Zone','FirstProdDate']], how=\"left\", left_on=\"i_uwi\", right_on=\"ChosenID\")\n",
    "\n",
    "    # Merge well attributes for `k_uwi_same1`, `k_uwi_same2`, `k_uwi_near1`, `k_uwi_near2`\n",
    "    header_colms = [\"ChosenID\", \"WellName\", \"RES_CAT\", \"Landing_Zone\", \"FirstProdDate\"]\n",
    "    well_pair_cols = [\"k_uwi_same1\", \"k_uwi_same2\", \"k_uwi_near1\", \"k_uwi_near2\"]\n",
    "\n",
    "    for col in well_pair_cols:\n",
    "        suffix = f\"_{col.split('_')[-1]}\"  # Extract full suffix (_same1, _same2, _near1, _near2)\n",
    "        final_df = final_df.merge(\n",
    "            header_sorted[header_colms],\n",
    "            how=\"left\",\n",
    "            left_on=col,\n",
    "            right_on=\"ChosenID\",\n",
    "            suffixes=(\"\", suffix)\n",
    "        )\n",
    "\n",
    "    # Convert categorical fields to `category` dtype for memory efficiency\n",
    "    for col in [\"WellName\", \"DSU\", \"RES_CAT\", \"Landing_Zone\"]:\n",
    "        final_df[col] = final_df[col].astype(\"category\")\n",
    "\n",
    "    # Convert numerical columns to `float32`\n",
    "    for col in [\"horizontal_dist_same1\", \"horizontal_dist_same2\", \"horizontal_dist_near1\", \"horizontal_dist_near2\"]:\n",
    "        final_df[col] = final_df[col].astype(np.float32)\n",
    "\n",
    "    # Remove redundant columns\n",
    "    final_df.drop(columns=[\"ChosenID\",\"ChosenID_same1\",\"ChosenID_same2\",\"ChosenID_near1\",\"ChosenID_near2\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = df_raw.copy()\n",
    "ik_pair = df_ik_pairs.copy()\n",
    "\n",
    "# Sort header for efficient merging\n",
    "header_sorted = header.sort_values(by=\"ChosenID\")\n",
    "\n",
    "# Merge Landing_Zone information only once\n",
    "df_merge = ik_pair.merge(\n",
    "    header_sorted[[\"ChosenID\", \"Landing_Zone\"]].rename(columns={'ChosenID': 'i_uwi'}),\n",
    "    how=\"left\", on=\"i_uwi\"\n",
    ").merge(\n",
    "    header_sorted[[\"ChosenID\", \"Landing_Zone\"]].rename(columns={'ChosenID': 'k_uwi'}),\n",
    "    how=\"left\", on=\"k_uwi\", suffixes=(\"_i\", \"_k\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the smallest `horizontal_dist` per (i_uwi, k_uwi)\n",
    "df_grouped_horiz = df_merge.groupby(['i_uwi', 'k_uwi', 'Landing_Zone_i', 'Landing_Zone_k'])[['horizontal_dist']].min().reset_index()\n",
    "\n",
    "# Keep only the smallest `vertical_dist` per (i_uwi, k_uwi)\n",
    "df_grouped_vert = df_merge.groupby(['i_uwi', 'k_uwi', 'Landing_Zone_i', 'Landing_Zone_k'])[['vertical_dist']].min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame efficiently\n",
    "df_sorted_horiz = df_grouped_horiz.sort_values(by=['i_uwi', 'horizontal_dist']).reset_index(drop=True)\n",
    "df_sorted_vert = df_grouped_vert.sort_values(by=['i_uwi', 'vertical_dist']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top 2 `k_uwi` values separately for same & different landing zones\n",
    "df_same = df_sorted_horiz[df_sorted_horiz[\"Landing_Zone_i\"] == df_sorted_horiz[\"Landing_Zone_k\"]].groupby(\"i_uwi\").head(2)\n",
    "\n",
    "df_diff = df_sorted_vert[df_sorted_vert[\"Landing_Zone_i\"] != df_sorted_vert[\"Landing_Zone_k\"]].groupby(\"i_uwi\").head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for both same and different Landing Zones\n",
    "def pivot_top_two(df, zone_type):\n",
    "    \"\"\"Helper function to pivot grouped data into `k_uwi_*`, `horizontal_dist_*`, and `vertical_dist_*` columns.\"\"\"\n",
    "    df[\"rank\"] = df.groupby(\"i_uwi\").cumcount() + 1  # Create ranking (1 or 2) for each i_uwi\n",
    "\n",
    "    # Identify available columns for pivoting\n",
    "    available_cols = [\"k_uwi\"] + list(df.columns.intersection([\"horizontal_dist\", \"vertical_dist\"]))\n",
    "\n",
    "    # Pivot only available columns\n",
    "    df_pivot = df.pivot(index=\"i_uwi\", columns=\"rank\", values=available_cols)\n",
    "\n",
    "    # Correct column renaming based on the landing zone type\n",
    "    df_pivot.columns = [\n",
    "        f\"k_uwi_{zone_type}{col[1]}\" if col[0] == \"k_uwi\"\n",
    "        else f\"horizontal_dist_{zone_type}{col[1]}\" if col[0] == \"horizontal_dist\"\n",
    "        else f\"vertical_dist_{zone_type}{col[1]}\"\n",
    "        for col in df_pivot.columns\n",
    "    ]\n",
    "\n",
    "    return df_pivot.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for both same and different Landing Zones\n",
    "df_same_pivot = pivot_top_two(df_same, \"same\")\n",
    "df_diff_pivot = pivot_top_two(df_diff, \"near\")\n",
    "\n",
    "final_df = df_same_pivot.merge(df_diff_pivot, how=\"outer\", on=\"i_uwi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging well attributes for `i_uwi`\n",
    "final_df = final_df.merge(header_sorted[['ChosenID','WellName','DSU','RES_CAT','Landing_Zone','FirstProdDate']].rename(columns={'ChosenID':'i_uwi'}), \n",
    "                          how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging well attributes for `k_uwi_same1`\n",
    "final_df = final_df.merge(header_sorted[['ChosenID','WellName','RES_CAT','Landing_Zone','FirstProdDate']].rename(columns={'ChosenID':'k_uwi_same1'}), \n",
    "                          how=\"left\", suffixes=(\"\", \"_same1\"), right_on=\"k_uwi_same1\", left_on=\"k_uwi_same1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging well attributes for , `k_uwi_same2`\n",
    "final_df = final_df.merge(header_sorted[['ChosenID','WellName','RES_CAT','Landing_Zone','FirstProdDate']].rename(columns={'ChosenID':'k_uwi_same2'}), \n",
    "                          how=\"left\", suffixes=(\"\", \"_same2\"), right_on=\"k_uwi_same2\", left_on=\"k_uwi_same2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging well attributes for `k_uwi_near1`\n",
    "final_df = final_df.merge(header_sorted[['ChosenID','WellName','RES_CAT','Landing_Zone','FirstProdDate']].rename(columns={'ChosenID':'k_uwi_near1'}), \n",
    "                          how=\"left\", suffixes=(\"\", \"_near1\"), right_on=\"k_uwi_near1\", left_on=\"k_uwi_near1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging well attributes for `k_uwi_near2`\n",
    "final_df = final_df.merge(header_sorted[['ChosenID','WellName','RES_CAT','Landing_Zone','FirstProdDate']].rename(columns={'ChosenID':'k_uwi_near2'}), \n",
    "                          how=\"left\", suffixes=(\"\", \"_near2\"), right_on=\"k_uwi_near2\", left_on=\"k_uwi_near2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging ik_pair\n",
    "final_df = final_df.merge(ik_pair[['i_uwi','k_uwi','vertical_dist','3D_ft_to_same']].rename(columns={\n",
    "    'k_uwi':'k_uwi_same1',\n",
    "    'vertical_dist':'vertical_dist_same1',\n",
    "    '3D_ft_to_same':'3D_ft_to_same1'\n",
    "}), \n",
    "               how='left', left_on=['i_uwi','k_uwi_same1'], right_on=['i_uwi','k_uwi_same1'], suffixes=(\"\", \"_same1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging ik_pair\n",
    "final_df = final_df.merge(ik_pair[['i_uwi','k_uwi','vertical_dist','3D_ft_to_same']].rename(columns={\n",
    "    'k_uwi':'k_uwi_same2',\n",
    "    'vertical_dist':'vertical_dist_same2',\n",
    "    '3D_ft_to_same':'3D_ft_to_same2'\n",
    "}), \n",
    "               how='left', left_on=['i_uwi','k_uwi_same2'], right_on=['i_uwi','k_uwi_same2'], suffixes=(\"\", \"_same2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging ik_pair\n",
    "final_df = final_df.merge(ik_pair[['i_uwi','k_uwi','horizontal_dist','3D_ft_to_same']].rename(columns={\n",
    "    'k_uwi':'k_uwi_near1',\n",
    "    'horizontal_dist':'horizontal_dist_near1',\n",
    "    '3D_ft_to_same':'3D_ft_to_near1'\n",
    "}), \n",
    "               how='left', left_on=['i_uwi','k_uwi_near1'], right_on=['i_uwi','k_uwi_near1'], suffixes=(\"\", \"_near1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging ik_pair\n",
    "final_df = final_df.merge(ik_pair[['i_uwi','k_uwi','horizontal_dist','3D_ft_to_same']].rename(columns={\n",
    "    'k_uwi':'k_uwi_near2',\n",
    "    'horizontal_dist':'horizontal_dist_near2',\n",
    "    '3D_ft_to_same':'3D_ft_to_near2'\n",
    "}), \n",
    "               how='left', left_on=['i_uwi','k_uwi_near2'], right_on=['i_uwi','k_uwi_near2'], suffixes=(\"\", \"_near2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i_uwi</th>\n",
       "      <th>WellName</th>\n",
       "      <th>DSU</th>\n",
       "      <th>RES_CAT</th>\n",
       "      <th>Landing_Zone</th>\n",
       "      <th>FirstProdDate</th>\n",
       "      <th>k_uwi_same1</th>\n",
       "      <th>WellName_same1</th>\n",
       "      <th>horizontal_dist_same1</th>\n",
       "      <th>vertical_dist_same1</th>\n",
       "      <th>3D_ft_to_same1</th>\n",
       "      <th>RES_CAT_same1</th>\n",
       "      <th>Landing_Zone_same1</th>\n",
       "      <th>FirstProdDate_same1</th>\n",
       "      <th>k_uwi_same2</th>\n",
       "      <th>WellName_same2</th>\n",
       "      <th>horizontal_dist_same2</th>\n",
       "      <th>vertical_dist_same2</th>\n",
       "      <th>3D_ft_to_same2</th>\n",
       "      <th>RES_CAT_same2</th>\n",
       "      <th>Landing_Zone_same2</th>\n",
       "      <th>FirstProdDate_same2</th>\n",
       "      <th>k_uwi_near1</th>\n",
       "      <th>WellName_near1</th>\n",
       "      <th>horizontal_dist_near1</th>\n",
       "      <th>vertical_dist_near1</th>\n",
       "      <th>3D_ft_to_near1</th>\n",
       "      <th>RES_CAT_near1</th>\n",
       "      <th>Landing_Zone_near1</th>\n",
       "      <th>FirstProdDate_near1</th>\n",
       "      <th>k_uwi_near2</th>\n",
       "      <th>WellName_near2</th>\n",
       "      <th>horizontal_dist_near2</th>\n",
       "      <th>vertical_dist_near2</th>\n",
       "      <th>3D_ft_to_near2</th>\n",
       "      <th>RES_CAT_near2</th>\n",
       "      <th>Landing_Zone_near2</th>\n",
       "      <th>FirstProdDate_near2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4238910251</td>\n",
       "      <td>BECKEN 11 1</td>\n",
       "      <td>BECKEN</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>1966-06-01</td>\n",
       "      <td>4238932199</td>\n",
       "      <td>RAPE `14` 1H</td>\n",
       "      <td>3293.977691</td>\n",
       "      <td>362.404537</td>\n",
       "      <td>3313.853659</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>2001-11-01</td>\n",
       "      <td>4238910422</td>\n",
       "      <td>TREES J C ESTATE ETAL 4H</td>\n",
       "      <td>7789.787864</td>\n",
       "      <td>497.902021</td>\n",
       "      <td>7805.683916</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>1967-05-01</td>\n",
       "      <td>4238932145</td>\n",
       "      <td>EVANS N T 10H</td>\n",
       "      <td>50706.339069</td>\n",
       "      <td>52.10879</td>\n",
       "      <td>50706.365844</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCA</td>\n",
       "      <td>1998-09-01</td>\n",
       "      <td>4238932273</td>\n",
       "      <td>HORRY PITTS 49 1H</td>\n",
       "      <td>45124.142926</td>\n",
       "      <td>189.848394</td>\n",
       "      <td>45124.542294</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>ELBG</td>\n",
       "      <td>2003-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4238910422</td>\n",
       "      <td>TREES J C ESTATE ETAL 4H</td>\n",
       "      <td>TREES J C ESTATE ETAL</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>1967-05-01</td>\n",
       "      <td>4238932199</td>\n",
       "      <td>RAPE `14` 1H</td>\n",
       "      <td>4648.667281</td>\n",
       "      <td>860.306558</td>\n",
       "      <td>4727.603501</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>2001-11-01</td>\n",
       "      <td>4238932244</td>\n",
       "      <td>BODKINS 12H</td>\n",
       "      <td>4849.931006</td>\n",
       "      <td>128.054030</td>\n",
       "      <td>4851.621234</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>2002-11-01</td>\n",
       "      <td>4238932270</td>\n",
       "      <td>EVANS N T 12HU</td>\n",
       "      <td>48205.994027</td>\n",
       "      <td>22.097478</td>\n",
       "      <td>48205.999092</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2003-09-01</td>\n",
       "      <td>4238932145</td>\n",
       "      <td>EVANS N T 10H</td>\n",
       "      <td>48156.367575</td>\n",
       "      <td>445.793231</td>\n",
       "      <td>48158.430929</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCA</td>\n",
       "      <td>1998-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4238910461</td>\n",
       "      <td>BARBER W T 8H</td>\n",
       "      <td>BARBER W T</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCA</td>\n",
       "      <td>1967-03-01</td>\n",
       "      <td>4238934872</td>\n",
       "      <td>LIGON STATE UNIT 7-22 1H</td>\n",
       "      <td>4902.723836</td>\n",
       "      <td>141.721362</td>\n",
       "      <td>4904.771754</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2015-09-01</td>\n",
       "      <td>4238935702</td>\n",
       "      <td>LIGON STATE UNIT 7-22 4313H</td>\n",
       "      <td>6681.482786</td>\n",
       "      <td>165.877161</td>\n",
       "      <td>6683.541535</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>4238937789</td>\n",
       "      <td>STATE SARAH LINK 3-2-6W 84H</td>\n",
       "      <td>51971.058159</td>\n",
       "      <td>0.12145</td>\n",
       "      <td>51971.058159</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>3RD BS</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>4238937634</td>\n",
       "      <td>OKINAWA 24-11 UNIT 1H</td>\n",
       "      <td>147604.690775</td>\n",
       "      <td>0.555667</td>\n",
       "      <td>147604.690776</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCB</td>\n",
       "      <td>2020-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4238910497</td>\n",
       "      <td>LIGON S E STATE 2H</td>\n",
       "      <td>LIGON S E STATE</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>1967-09-01</td>\n",
       "      <td>4238932286</td>\n",
       "      <td>WAHLENMAIER STATE 5H</td>\n",
       "      <td>5523.86989</td>\n",
       "      <td>357.112593</td>\n",
       "      <td>5535.401337</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>2004-10-01</td>\n",
       "      <td>4238930509</td>\n",
       "      <td>LIGON S E 18 71H</td>\n",
       "      <td>5987.869496</td>\n",
       "      <td>166.379576</td>\n",
       "      <td>5990.180570</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>1977-12-01</td>\n",
       "      <td>4238932273</td>\n",
       "      <td>HORRY PITTS 49 1H</td>\n",
       "      <td>22208.249435</td>\n",
       "      <td>57.352557</td>\n",
       "      <td>22208.323491</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>ELBG</td>\n",
       "      <td>2003-12-01</td>\n",
       "      <td>4238932231</td>\n",
       "      <td>LIGON S E `20` 1H</td>\n",
       "      <td>11686.026961</td>\n",
       "      <td>267.913295</td>\n",
       "      <td>11689.097641</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>ELLENBURGER</td>\n",
       "      <td>2002-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4238910511</td>\n",
       "      <td>CURRY UNIT 1H</td>\n",
       "      <td>CURRY UNIT</td>\n",
       "      <td>02PDNP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>2006-08-01</td>\n",
       "      <td>4238932252</td>\n",
       "      <td>BEEFMASTER 1H</td>\n",
       "      <td>4516.825198</td>\n",
       "      <td>831.808042</td>\n",
       "      <td>4592.778515</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>2003-04-01</td>\n",
       "      <td>4238932220</td>\n",
       "      <td>MONSANTO MCKELLER UNIT 2H</td>\n",
       "      <td>6820.048813</td>\n",
       "      <td>319.219475</td>\n",
       "      <td>6827.515425</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>DEVONIAN</td>\n",
       "      <td>2002-03-01</td>\n",
       "      <td>4238932290</td>\n",
       "      <td>TEXACO 17 1H</td>\n",
       "      <td>85401.035382</td>\n",
       "      <td>240.904996</td>\n",
       "      <td>85401.375162</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2004-08-01</td>\n",
       "      <td>4238932231</td>\n",
       "      <td>LIGON S E `20` 1H</td>\n",
       "      <td>26781.435509</td>\n",
       "      <td>730.342235</td>\n",
       "      <td>26791.392045</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>ELLENBURGER</td>\n",
       "      <td>2002-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>4238941111</td>\n",
       "      <td>COWHORN UNIT 208-209 22H</td>\n",
       "      <td>COWHORN UNIT</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>4238941109</td>\n",
       "      <td>COWHORN UNIT 208-209 41H</td>\n",
       "      <td>600.653691</td>\n",
       "      <td>329.248149</td>\n",
       "      <td>684.973869</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>4238941112</td>\n",
       "      <td>COWHORN UNIT 208-209 42H</td>\n",
       "      <td>723.662275</td>\n",
       "      <td>391.478804</td>\n",
       "      <td>822.765301</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>4238940015</td>\n",
       "      <td>REV WOR 3 / 7 0014TB</td>\n",
       "      <td>92829.506376</td>\n",
       "      <td>1.158085</td>\n",
       "      <td>92829.506383</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>3RD BS</td>\n",
       "      <td>2023-08-01</td>\n",
       "      <td>4238940180</td>\n",
       "      <td>ASTRO STATE 51-7-22-15 D 82H</td>\n",
       "      <td>42600.039907</td>\n",
       "      <td>1.825718</td>\n",
       "      <td>42600.039946</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>3RD BS</td>\n",
       "      <td>2022-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>4238941112</td>\n",
       "      <td>COWHORN UNIT 208-209 42H</td>\n",
       "      <td>COWHORN UNIT</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>4238941111</td>\n",
       "      <td>COWHORN UNIT 208-209 22H</td>\n",
       "      <td>723.662275</td>\n",
       "      <td>391.478804</td>\n",
       "      <td>822.765301</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>4238941113</td>\n",
       "      <td>COWHORN UNIT 208-209 23H</td>\n",
       "      <td>851.371569</td>\n",
       "      <td>358.628822</td>\n",
       "      <td>923.822592</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>4238932977</td>\n",
       "      <td>PHANTOM-BURKHOLDER UNIT 1H</td>\n",
       "      <td>110647.954025</td>\n",
       "      <td>1.146117</td>\n",
       "      <td>110647.954031</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCXY</td>\n",
       "      <td>2012-02-01</td>\n",
       "      <td>4238935378</td>\n",
       "      <td>REEVES STATE T7-50-30 11H</td>\n",
       "      <td>52984.470250</td>\n",
       "      <td>2.948522</td>\n",
       "      <td>52984.470332</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCB</td>\n",
       "      <td>2017-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>4238941113</td>\n",
       "      <td>COWHORN UNIT 208-209 23H</td>\n",
       "      <td>COWHORN UNIT</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>4238941112</td>\n",
       "      <td>COWHORN UNIT 208-209 42H</td>\n",
       "      <td>851.371569</td>\n",
       "      <td>358.628822</td>\n",
       "      <td>923.822592</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>4238941111</td>\n",
       "      <td>COWHORN UNIT 208-209 22H</td>\n",
       "      <td>1327.16517</td>\n",
       "      <td>32.849981</td>\n",
       "      <td>1327.571659</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-11-01</td>\n",
       "      <td>4238938554</td>\n",
       "      <td>GIGI STATE 12-13-1N 81H</td>\n",
       "      <td>51203.323118</td>\n",
       "      <td>1.578018</td>\n",
       "      <td>51203.323142</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>3RD BS</td>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>4238938216</td>\n",
       "      <td>CORIANDER 2524-C3 13H</td>\n",
       "      <td>149974.586471</td>\n",
       "      <td>1.614609</td>\n",
       "      <td>149974.586479</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>3RD BS</td>\n",
       "      <td>2019-08-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>4238941137</td>\n",
       "      <td>MOSAIC STATE UNIT 33-34 6TB</td>\n",
       "      <td>MOSAIC STATE UNIT</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>3RD BS</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>4238939888</td>\n",
       "      <td>MOSAIC STATE UNIT 33-34 12H</td>\n",
       "      <td>2981.493141</td>\n",
       "      <td>57.010724</td>\n",
       "      <td>2982.038157</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>3RD BS</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>4238938522</td>\n",
       "      <td>MOSAIC STATE UNIT 33-34 2H</td>\n",
       "      <td>4256.020256</td>\n",
       "      <td>138.528638</td>\n",
       "      <td>4258.274134</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>3RD BS</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>4238937548</td>\n",
       "      <td>DOC MARTENS UNIT U 03H</td>\n",
       "      <td>81239.575098</td>\n",
       "      <td>0.214725</td>\n",
       "      <td>81239.575099</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>4238939896</td>\n",
       "      <td>NOEL 313-312-311 C 12A</td>\n",
       "      <td>90673.318263</td>\n",
       "      <td>0.519266</td>\n",
       "      <td>90673.318264</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2023-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>4238941155</td>\n",
       "      <td>REV CON T8-51-7 D 0024WA</td>\n",
       "      <td>REV CON T</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>4238940601</td>\n",
       "      <td>REV CON T8-51-7 C 0013WA</td>\n",
       "      <td>1587.467109</td>\n",
       "      <td>0.974519</td>\n",
       "      <td>1587.467408</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>4238940600</td>\n",
       "      <td>REV CON T8-51-7 B 0012WA</td>\n",
       "      <td>3138.584279</td>\n",
       "      <td>12.142145</td>\n",
       "      <td>3138.607766</td>\n",
       "      <td>03PUD</td>\n",
       "      <td>WCA</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>4238938306</td>\n",
       "      <td>TASHA 17-20A 20H</td>\n",
       "      <td>109193.306923</td>\n",
       "      <td>0.334396</td>\n",
       "      <td>109193.306924</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>WCC</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>4238936645</td>\n",
       "      <td>RED ROCK B T 09H</td>\n",
       "      <td>93057.448487</td>\n",
       "      <td>1.073076</td>\n",
       "      <td>93057.448493</td>\n",
       "      <td>01PDP</td>\n",
       "      <td>3RD BS</td>\n",
       "      <td>2018-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2484 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           i_uwi                     WellName                    DSU RES_CAT  \\\n",
       "0     4238910251                  BECKEN 11 1                 BECKEN   01PDP   \n",
       "1     4238910422     TREES J C ESTATE ETAL 4H  TREES J C ESTATE ETAL   01PDP   \n",
       "2     4238910461                BARBER W T 8H             BARBER W T   01PDP   \n",
       "3     4238910497           LIGON S E STATE 2H        LIGON S E STATE   01PDP   \n",
       "4     4238910511                CURRY UNIT 1H             CURRY UNIT  02PDNP   \n",
       "...          ...                          ...                    ...     ...   \n",
       "2479  4238941111     COWHORN UNIT 208-209 22H           COWHORN UNIT   03PUD   \n",
       "2480  4238941112     COWHORN UNIT 208-209 42H           COWHORN UNIT   03PUD   \n",
       "2481  4238941113     COWHORN UNIT 208-209 23H           COWHORN UNIT   03PUD   \n",
       "2482  4238941137  MOSAIC STATE UNIT 33-34 6TB      MOSAIC STATE UNIT   03PUD   \n",
       "2483  4238941155     REV CON T8-51-7 D 0024WA              REV CON T   03PUD   \n",
       "\n",
       "     Landing_Zone FirstProdDate k_uwi_same1               WellName_same1  \\\n",
       "0        DEVONIAN    1966-06-01  4238932199                 RAPE `14` 1H   \n",
       "1        DEVONIAN    1967-05-01  4238932199                 RAPE `14` 1H   \n",
       "2             WCA    1967-03-01  4238934872     LIGON STATE UNIT 7-22 1H   \n",
       "3        DEVONIAN    1967-09-01  4238932286         WAHLENMAIER STATE 5H   \n",
       "4        DEVONIAN    2006-08-01  4238932252                BEEFMASTER 1H   \n",
       "...           ...           ...         ...                          ...   \n",
       "2479          WCA    2024-11-01  4238941109     COWHORN UNIT 208-209 41H   \n",
       "2480          WCA    2024-12-01  4238941111     COWHORN UNIT 208-209 22H   \n",
       "2481          WCA    2024-12-01  4238941112     COWHORN UNIT 208-209 42H   \n",
       "2482       3RD BS    2025-01-01  4238939888  MOSAIC STATE UNIT 33-34 12H   \n",
       "2483          WCA    2024-12-01  4238940601     REV CON T8-51-7 C 0013WA   \n",
       "\n",
       "     horizontal_dist_same1  vertical_dist_same1  3D_ft_to_same1 RES_CAT_same1  \\\n",
       "0              3293.977691           362.404537     3313.853659         01PDP   \n",
       "1              4648.667281           860.306558     4727.603501         01PDP   \n",
       "2              4902.723836           141.721362     4904.771754         01PDP   \n",
       "3               5523.86989           357.112593     5535.401337         01PDP   \n",
       "4              4516.825198           831.808042     4592.778515         01PDP   \n",
       "...                    ...                  ...             ...           ...   \n",
       "2479            600.653691           329.248149      684.973869         03PUD   \n",
       "2480            723.662275           391.478804      822.765301         03PUD   \n",
       "2481            851.371569           358.628822      923.822592         03PUD   \n",
       "2482           2981.493141            57.010724     2982.038157         01PDP   \n",
       "2483           1587.467109             0.974519     1587.467408         03PUD   \n",
       "\n",
       "     Landing_Zone_same1 FirstProdDate_same1 k_uwi_same2  \\\n",
       "0              DEVONIAN          2001-11-01  4238910422   \n",
       "1              DEVONIAN          2001-11-01  4238932244   \n",
       "2                   WCA          2015-09-01  4238935702   \n",
       "3              DEVONIAN          2004-10-01  4238930509   \n",
       "4              DEVONIAN          2003-04-01  4238932220   \n",
       "...                 ...                 ...         ...   \n",
       "2479                WCA          2024-11-01  4238941112   \n",
       "2480                WCA          2024-11-01  4238941113   \n",
       "2481                WCA          2024-12-01  4238941111   \n",
       "2482             3RD BS          2022-12-01  4238938522   \n",
       "2483                WCA          2024-07-01  4238940600   \n",
       "\n",
       "                   WellName_same2 horizontal_dist_same2  vertical_dist_same2  \\\n",
       "0        TREES J C ESTATE ETAL 4H           7789.787864           497.902021   \n",
       "1                     BODKINS 12H           4849.931006           128.054030   \n",
       "2     LIGON STATE UNIT 7-22 4313H           6681.482786           165.877161   \n",
       "3                LIGON S E 18 71H           5987.869496           166.379576   \n",
       "4       MONSANTO MCKELLER UNIT 2H           6820.048813           319.219475   \n",
       "...                           ...                   ...                  ...   \n",
       "2479     COWHORN UNIT 208-209 42H            723.662275           391.478804   \n",
       "2480     COWHORN UNIT 208-209 23H            851.371569           358.628822   \n",
       "2481     COWHORN UNIT 208-209 22H            1327.16517            32.849981   \n",
       "2482   MOSAIC STATE UNIT 33-34 2H           4256.020256           138.528638   \n",
       "2483     REV CON T8-51-7 B 0012WA           3138.584279            12.142145   \n",
       "\n",
       "      3D_ft_to_same2 RES_CAT_same2 Landing_Zone_same2 FirstProdDate_same2  \\\n",
       "0        7805.683916         01PDP           DEVONIAN          1967-05-01   \n",
       "1        4851.621234         01PDP           DEVONIAN          2002-11-01   \n",
       "2        6683.541535         01PDP                WCA          2018-03-01   \n",
       "3        5990.180570         01PDP           DEVONIAN          1977-12-01   \n",
       "4        6827.515425         01PDP           DEVONIAN          2002-03-01   \n",
       "...              ...           ...                ...                 ...   \n",
       "2479      822.765301         03PUD                WCA          2024-12-01   \n",
       "2480      923.822592         03PUD                WCA          2024-12-01   \n",
       "2481     1327.571659         03PUD                WCA          2024-11-01   \n",
       "2482     4258.274134         01PDP             3RD BS          2020-01-01   \n",
       "2483     3138.607766         03PUD                WCA          2024-07-01   \n",
       "\n",
       "     k_uwi_near1               WellName_near1  horizontal_dist_near1  \\\n",
       "0     4238932145                EVANS N T 10H           50706.339069   \n",
       "1     4238932270               EVANS N T 12HU           48205.994027   \n",
       "2     4238937789  STATE SARAH LINK 3-2-6W 84H           51971.058159   \n",
       "3     4238932273            HORRY PITTS 49 1H           22208.249435   \n",
       "4     4238932290                 TEXACO 17 1H           85401.035382   \n",
       "...          ...                          ...                    ...   \n",
       "2479  4238940015         REV WOR 3 / 7 0014TB           92829.506376   \n",
       "2480  4238932977   PHANTOM-BURKHOLDER UNIT 1H          110647.954025   \n",
       "2481  4238938554      GIGI STATE 12-13-1N 81H           51203.323118   \n",
       "2482  4238937548       DOC MARTENS UNIT U 03H           81239.575098   \n",
       "2483  4238938306             TASHA 17-20A 20H          109193.306923   \n",
       "\n",
       "     vertical_dist_near1  3D_ft_to_near1 RES_CAT_near1 Landing_Zone_near1  \\\n",
       "0               52.10879    50706.365844         01PDP                WCA   \n",
       "1              22.097478    48205.999092         01PDP                WCA   \n",
       "2                0.12145    51971.058159         01PDP             3RD BS   \n",
       "3              57.352557    22208.323491         01PDP               ELBG   \n",
       "4             240.904996    85401.375162         01PDP                WCA   \n",
       "...                  ...             ...           ...                ...   \n",
       "2479            1.158085    92829.506383         01PDP             3RD BS   \n",
       "2480            1.146117   110647.954031         01PDP               WCXY   \n",
       "2481            1.578018    51203.323142         01PDP             3RD BS   \n",
       "2482            0.214725    81239.575099         01PDP                WCA   \n",
       "2483            0.334396   109193.306924         01PDP                WCC   \n",
       "\n",
       "     FirstProdDate_near1 k_uwi_near2                WellName_near2  \\\n",
       "0             1998-09-01  4238932273             HORRY PITTS 49 1H   \n",
       "1             2003-09-01  4238932145                 EVANS N T 10H   \n",
       "2             2019-04-01  4238937634         OKINAWA 24-11 UNIT 1H   \n",
       "3             2003-12-01  4238932231             LIGON S E `20` 1H   \n",
       "4             2004-08-01  4238932231             LIGON S E `20` 1H   \n",
       "...                  ...         ...                           ...   \n",
       "2479          2023-08-01  4238940180  ASTRO STATE 51-7-22-15 D 82H   \n",
       "2480          2012-02-01  4238935378     REEVES STATE T7-50-30 11H   \n",
       "2481          2019-12-01  4238938216         CORIANDER 2524-C3 13H   \n",
       "2482          2019-08-01  4238939896        NOEL 313-312-311 C 12A   \n",
       "2483          2019-08-01  4238936645              RED ROCK B T 09H   \n",
       "\n",
       "      horizontal_dist_near2 vertical_dist_near2  3D_ft_to_near2 RES_CAT_near2  \\\n",
       "0              45124.142926          189.848394    45124.542294         01PDP   \n",
       "1              48156.367575          445.793231    48158.430929         01PDP   \n",
       "2             147604.690775            0.555667   147604.690776         01PDP   \n",
       "3              11686.026961          267.913295    11689.097641         01PDP   \n",
       "4              26781.435509          730.342235    26791.392045         01PDP   \n",
       "...                     ...                 ...             ...           ...   \n",
       "2479           42600.039907            1.825718    42600.039946         01PDP   \n",
       "2480           52984.470250            2.948522    52984.470332         01PDP   \n",
       "2481          149974.586471            1.614609   149974.586479         01PDP   \n",
       "2482           90673.318263            0.519266    90673.318264         01PDP   \n",
       "2483           93057.448487            1.073076    93057.448493         01PDP   \n",
       "\n",
       "     Landing_Zone_near2 FirstProdDate_near2  \n",
       "0                  ELBG          2003-12-01  \n",
       "1                   WCA          1998-09-01  \n",
       "2                   WCB          2020-01-01  \n",
       "3           ELLENBURGER          2002-09-01  \n",
       "4           ELLENBURGER          2002-09-01  \n",
       "...                 ...                 ...  \n",
       "2479             3RD BS          2022-12-01  \n",
       "2480                WCB          2017-09-01  \n",
       "2481             3RD BS          2019-08-01  \n",
       "2482                WCA          2023-03-01  \n",
       "2483             3RD BS          2018-06-01  \n",
       "\n",
       "[2484 rows x 38 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reorder_columns(final_df,['WellName', 'DSU', 'RES_CAT', 'Landing_Zone', 'FirstProdDate',\n",
    "                          'k_uwi_same1','WellName_same1','horizontal_dist_same1','vertical_dist_same1','3D_ft_to_same1','RES_CAT_same1', 'Landing_Zone_same1', 'FirstProdDate_same1',\n",
    "                         'k_uwi_same2','WellName_same2','horizontal_dist_same2','vertical_dist_same2','3D_ft_to_same2', 'RES_CAT_same2', 'Landing_Zone_same2', 'FirstProdDate_same2',\n",
    "                         'k_uwi_near1','WellName_near1','horizontal_dist_near1','vertical_dist_near1','3D_ft_to_near1', 'RES_CAT_near1', 'Landing_Zone_near1', 'FirstProdDate_near1',\n",
    "                         'k_uwi_near2','WellName_near2','horizontal_dist_near2','vertical_dist_near2','3D_ft_to_near2', 'RES_CAT_near2', 'Landing_Zone_near2', 'FirstProdDate_near2'],\n",
    "                         reference_column='i_uwi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
