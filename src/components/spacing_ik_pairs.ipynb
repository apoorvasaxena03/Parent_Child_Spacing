{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Well Bundles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing / Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Importing pandas package\n",
    "\n",
    "# Set the maximum number of columns to display to None\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy as np # Importing numpy package\n",
    "\n",
    "from typing import Dict, Tuple, List, Union # Importing specific types from typing module\n",
    "\n",
    "import re # Importing regular expression package\n",
    "\n",
    "from src.database_manager import DatabricksOdbcConnector # Importing DatabricksOdbcConnector class from database_manager module\n",
    "from src.utils import reorder_columns # Importing reorder_columns function from utils module\n",
    "\n",
    "from scipy.spatial.distance import cdist # Importing cdist function from scipy package\n",
    "\n",
    "import time\n",
    "\n",
    "import pyproj # Importing pyproj package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Excel/csv into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('wellHeader_with_Cluster.csv',dtype={'ChosenID':str},parse_dates=['FirstProdDate','Comp_Dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming cluster column to bundle\n",
    "df_raw = df_raw.rename(columns={'cluster':'bundle'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by a specific column (e.g., 'bundle') in ascending order\n",
    "df_raw.sort_values(by='bundle', ascending=True, ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2514, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Creating DSU Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DSU columns names from Lease Name columns\n",
    "\n",
    "df_raw['DSU'] = df_raw['LeaseName'].apply(\n",
    "    lambda x: re.sub(r'[^a-zA-Z\\s]', ' ',  # Remove special characters, keep letters and spaces\n",
    "                     re.match(r'([^\\d]+)', str(x)).group(1) if pd.notna(x) and re.match(r'([^\\d]+)', str(x)) else str(x))  \n",
    "                    .strip()  # Strip leading/trailing spaces\n",
    ").replace(r'\\s+', ' ', regex=True)  # Collapse multiple spaces into a single space\n",
    "\n",
    "# Placing DSU next to LeaseName\n",
    "df_raw = reorder_columns(df=df_raw, columns_to_move=['DSU'], reference_column='LeaseName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Creating dataframes that have more than one unique bundles or DSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the same DSU has more than one unique bundle\n",
    "same_DSU_diffBundle_df = df_raw[df_raw.groupby(\"DSU\")[\"bundle\"].transform(\"nunique\") > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the same bundle has more than one unique DSU\n",
    "same_Bundle_diffDSU_df = df_raw[df_raw.groupby(\"bundle\")[\"DSU\"].transform(\"nunique\") > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Defining Functions that is used in calculation for i-k pair dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_heel_toe_mid_lat_lon(well_trajectory: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the heel, toe, and mid-point latitude/longitude for each ChosenID in the well trajectory DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    well_trajectory: pd.DataFrame\n",
    "        DataFrame containing well trajectory data, including 'ChosenID', 'md', 'latitude', and 'longitude'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        A DataFrame with 'ChosenID', 'Heel_Lat', 'Heel_Lon', 'Toe_Lat', 'Toe_Lon', 'Mid_Lat', 'Mid_Lon'.\n",
    "\n",
    "    Example:\n",
    "    >>> data = {\n",
    "    ...     \"ChosenID\": [1001, 1001, 1001, 1002, 1002],\n",
    "    ...     \"md\": [5000, 5100, 5200, 6000, 6100],\n",
    "    ...     \"latitude\": [31.388, 31.389, 31.387, 31.400, 31.401],\n",
    "    ...     \"longitude\": [-103.314, -103.315, -103.316, -103.318, -103.319]\n",
    "    ... }\n",
    "    >>> df = pd.DataFrame(data)\n",
    "    >>> extract_heel_toe_mid_lat_lon(df)\n",
    "       ChosenID  Heel_Lat  Heel_Lon  Toe_Lat  Toe_Lon  Mid_Lat  Mid_Lon\n",
    "    0     1001    31.388  -103.314   31.387  -103.316  31.3875 -103.315\n",
    "    1     1002    31.400  -103.318   31.401  -103.319  31.4005 -103.3185\n",
    "    \"\"\"\n",
    "    # Ensure the data is sorted by MD in ascending order\n",
    "    well_trajectory = well_trajectory.sort_values(by=[\"ChosenID\", \"md\"], ascending=True)\n",
    "\n",
    "    # Group by 'ChosenID' and extract heel/toe lat/lon\n",
    "    heel_toe_df = (\n",
    "        well_trajectory.groupby(\"ChosenID\")\n",
    "        .agg(\n",
    "            heel_lat=(\"latitude\", \"first\"),\n",
    "            heel_lon=(\"longitude\", \"first\"),\n",
    "            toe_lat=(\"latitude\", \"last\"),\n",
    "            toe_lon=(\"longitude\", \"last\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calculate midpoints\n",
    "    heel_toe_df[\"mid_Lat\"] = (heel_toe_df[\"heel_lat\"] + heel_toe_df[\"toe_lat\"]) / 2\n",
    "    heel_toe_df[\"mid_Lon\"] = (heel_toe_df[\"heel_lon\"] + heel_toe_df[\"toe_lon\"]) / 2\n",
    "\n",
    "    return heel_toe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_direction(lat1: np.ndarray, lon1: np.ndarray, lat2: np.ndarray, lon2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Determine the relative direction of (lat2, lon2) with respect to (lat1, lon1).\n",
    "    \n",
    "    Parameters:\n",
    "    lat1, lon1: np.ndarray\n",
    "        Latitude and longitude of the first well.\n",
    "    lat2, lon2: np.ndarray\n",
    "        Latitude and longitude of the second well.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray\n",
    "        Array indicating the direction (e.g., North, South, East, West) of well B relative to well A.\n",
    "    \"\"\"\n",
    "    lat_diff = lat2 - lat1\n",
    "    lon_diff = lon2 - lon1\n",
    "\n",
    "    conditions = [\n",
    "        np.abs(lat_diff) > np.abs(lon_diff),\n",
    "        lat_diff > 0,\n",
    "        lon_diff > 0\n",
    "    ]\n",
    "\n",
    "    choices = [\"North\", \"South\", \"East\", \"West\"]\n",
    "    \n",
    "    return np.select(\n",
    "        [conditions[0] & conditions[1], conditions[0] & ~conditions[1], ~conditions[0] & conditions[2], ~conditions[0] & ~conditions[2]],\n",
    "        choices\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_drill_direction_vectorized(well_trajectories: Dict[str, pd.DataFrame], i_indices: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized function to determine the drilling direction of multiple wells using NumPy operations.\n",
    "    \n",
    "    Parameters:\n",
    "    well_trajectories: Dict[str, pd.DataFrame]\n",
    "        Dictionary containing well trajectory data indexed by ChosenID.\n",
    "    i_indices: np.ndarray\n",
    "        Array of ChosenID whose drill directions need to be calculated.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray\n",
    "        Array containing \"EW\" (East-West) or \"NS\" (North-South) for each well.\n",
    "    \"\"\"\n",
    "    azimuth_values = np.array([well_trajectories[i][\"azimuth\"].mean() if not well_trajectories[i].empty else np.nan for i in i_indices])\n",
    "    \n",
    "    conditions = (45 <= azimuth_values) & (azimuth_values < 135) | (225 <= azimuth_values) & (azimuth_values < 315)\n",
    "    drill_directions = np.where(np.isnan(azimuth_values), \"Unknown\", np.where(conditions, \"EW\", \"NS\"))\n",
    "    \n",
    "    return drill_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_calculate_3D_distance_matrix(\n",
    "    trajectories: Dict[str, pd.DataFrame], i_indices: np.ndarray, k_indices: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Fully vectorized 3D distance calculations for well pairs using NumPy and Pandas.\n",
    "    \n",
    "    Parameters:\n",
    "    trajectories: Dict[str, pd.DataFrame]\n",
    "        Dictionary containing well trajectory data indexed by well ID.\n",
    "    i_indices: np.ndarray\n",
    "        Array of well IDs representing the first well in each pair.\n",
    "    k_indices: np.ndarray\n",
    "        Array of well IDs representing the second well in each pair.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        - Horizontal distances between the well pairs.\n",
    "        - Vertical distances between the well pairs.\n",
    "        - 3D distances between the well pairs.\n",
    "    \"\"\"\n",
    "    # ðŸš€ Precompute mean (midpoint) for each well ID across all wells at once\n",
    "    all_trajectories_df = pd.concat(trajectories.values(), keys=trajectories.keys()).reset_index(drop=True)\n",
    "\n",
    "    midpoints_df = all_trajectories_df.groupby(\"ChosenID\")[[\"x\", \"y\", \"tvd\"]].mean()\n",
    "\n",
    "    # Convert to NumPy arrays for fast lookup\n",
    "    well_ids = midpoints_df.index.to_numpy()\n",
    "    midpoints = midpoints_df.to_numpy()\n",
    "\n",
    "    # Create a mapping from well ID to its index\n",
    "    well_id_to_idx = {well_id: idx for idx, well_id in enumerate(well_ids)}\n",
    "\n",
    "    # Efficiently extract midpoints using NumPy indexing\n",
    "    mid_A = midpoints[np.array([well_id_to_idx[i] for i in i_indices])]\n",
    "    mid_B = midpoints[np.array([well_id_to_idx[k] for k in k_indices])]\n",
    "\n",
    "    # Compute distances\n",
    "    vertical_distances = np.abs(mid_A[:, 2] - mid_B[:, 2])\n",
    "    mid_B[:, 2] = mid_A[:, 2]  # Align Well B to Well Aâ€™s TVD\n",
    "\n",
    "    horizontal_distances = np.linalg.norm(mid_A[:, :2] - mid_B[:, :2], axis=1)\n",
    "    total_3D_distances = np.sqrt(horizontal_distances**2 + vertical_distances**2)\n",
    "\n",
    "    return horizontal_distances, vertical_distances, total_3D_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_i_k_pairs(df: pd.DataFrame, trajectories: Union[Dict[str, pd.DataFrame], pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate the i_k_pairs DataFrame, computing horizontal and vertical distances, \n",
    "    3D distances, drilling directions, and relative directions between well pairs.\n",
    "    \n",
    "    Parameters:\n",
    "    df: pd.DataFrame\n",
    "        DataFrame containing well metadata with:\n",
    "        - \"ChosenID\" (str): Unique well identifier.\n",
    "\n",
    "    trajectories: Union[Dict[str, pd.DataFrame], pd.DataFrame]\n",
    "        Either:\n",
    "        - A dictionary mapping well IDs (\"ChosenID\") to trajectory DataFrames.\n",
    "        - A single DataFrame containing all trajectory data (must have \"ChosenID\" column).\n",
    "        \n",
    "    Each trajectory DataFrame should include:\n",
    "    - \"md\" (float): Measured depth.\n",
    "    - \"tvd\" (float): True vertical depth.\n",
    "    - \"inclination\" (float): Inclination angle in degrees.\n",
    "    - \"azimuth\" (float): represents the drilling direction.\n",
    "    - \"latitude\" (float): Latitude values, define the geographical position.\n",
    "    - \"longitude\" (float): Longitude values, define the geographical position.\n",
    "    - \"x\" (float): X-coordinate in a Cartesian coordinate system.\n",
    "    - \"y\" (float): Y-coordinate in a Cartesian coordinate system.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        DataFrame containing pairs of wells (`i_uwi`, `k_uwi`) with their computed distances \n",
    "        and directional relationships.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Convert to dictionary if input is a DataFrame\n",
    "    if isinstance(trajectories, pd.DataFrame):\n",
    "        if \"ChosenID\" not in trajectories.columns:\n",
    "            raise ValueError(\"ðŸš¨ Error: Trajectory DataFrame must contain a 'ChosenID' column.\")\n",
    "        trajectories = {cid: group for cid, group in trajectories.groupby(\"ChosenID\")}\n",
    "\n",
    "    step1_time = time.time()\n",
    "    print(f\"âœ… Step 1: Trajectory DataFrame conversion took {step1_time - start_time:.4f} seconds.\")\n",
    "\n",
    "    # Get unique ChosenIDs from df and validate existence in trajectories\n",
    "    chosen_ids = df[\"ChosenID\"].unique()\n",
    "    missing_ids = [cid for cid in chosen_ids if cid not in trajectories]\n",
    "\n",
    "    if missing_ids:\n",
    "        print(f\"âš ï¸ The following ChosenIDs do not exist in the trajectory data and will be excluded: {missing_ids}\")\n",
    "\n",
    "    df = df[df[\"ChosenID\"].isin(trajectories)]\n",
    "    \n",
    "    step2_time = time.time()\n",
    "    print(f\"âœ… Step 2: ChosenID validation and filtering took {step2_time - step1_time:.4f} seconds.\")\n",
    "\n",
    "    # Generate i-k pair indices\n",
    "    n = len(df)\n",
    "    i_idx, k_idx = np.triu_indices(n, k=1)\n",
    "\n",
    "    i_uwi = df.iloc[i_idx][\"ChosenID\"].values\n",
    "    k_uwi = df.iloc[k_idx][\"ChosenID\"].values\n",
    "\n",
    "    step3_time = time.time()\n",
    "    print(f\"âœ… Step 3: i-k pair generation took {step3_time - step2_time:.4f} seconds.\")\n",
    "\n",
    "    # ðŸš€ Optimized Heel/Toe Extraction (Vectorized)\n",
    "    heel_toe_df = pd.concat(\n",
    "        [extract_heel_toe_mid_lat_lon(trajectories[cid]) for cid in df['ChosenID'].unique()], ignore_index=True\n",
    "    )\n",
    "    heel_toe_dict = heel_toe_df.set_index(\"ChosenID\").to_dict(orient=\"index\")\n",
    "\n",
    "    step4_time = time.time()\n",
    "    print(f\"âœ… Step 4 (Optimized): Heel/Toe extraction took {step4_time - step3_time:.4f} seconds.\")\n",
    "\n",
    "    # Efficiently extract values using vectorized lookups\n",
    "    heel_lat_i = np.array([heel_toe_dict[i][\"heel_lat\"] for i in i_uwi])\n",
    "    heel_lon_i = np.array([heel_toe_dict[i][\"heel_lon\"] for i in i_uwi])\n",
    "    toe_lat_k = np.array([heel_toe_dict[k][\"toe_lat\"] for k in k_uwi])\n",
    "    toe_lon_k = np.array([heel_toe_dict[k][\"toe_lon\"] for k in k_uwi])\n",
    "\n",
    "    step5_time = time.time()\n",
    "    print(f\"âœ… Step 5: Heel/Toe dictionary lookup took {step5_time - step4_time:.4f} seconds.\")\n",
    "\n",
    "    # ðŸš€ Optimized Distance Calculation (Fully Vectorized)\n",
    "    horizontal_dist, vertical_dist, total_3D_dist = optimized_calculate_3D_distance_matrix(trajectories, i_uwi, k_uwi)\n",
    "\n",
    "    step6_time = time.time()\n",
    "    print(f\"âœ… Step 6 (Optimized): Distance calculations took {step6_time - step5_time:.4f} seconds.\")\n",
    "\n",
    "    # Compute drill directions\n",
    "    drill_directions = calculate_drill_direction_vectorized(trajectories, i_uwi)\n",
    "\n",
    "    step7_time = time.time()\n",
    "    print(f\"âœ… Step 7: Drill direction calculation took {step7_time - step6_time:.4f} seconds.\")\n",
    "\n",
    "    # Determine directional relationship\n",
    "    ward_of_i = get_direction(heel_lat_i, heel_lon_i, toe_lat_k, toe_lon_k)\n",
    "\n",
    "    step8_time = time.time()\n",
    "    print(f\"âœ… Step 8: Directional relationship calculation took {step8_time - step7_time:.4f} seconds.\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        \"i_uwi\": i_uwi,\n",
    "        \"k_uwi\": k_uwi,\n",
    "        \"horizontal_dist\": horizontal_dist,\n",
    "        \"vertical_dist\": vertical_dist,\n",
    "        \"3D_ft_to_same\": total_3D_dist,\n",
    "        \"drill_direction\": drill_directions,\n",
    "        \"ward_of_i\": ward_of_i\n",
    "    })\n",
    "\n",
    "    total_time = time.time()\n",
    "    print(f\"ðŸš€ Total Execution Time: {total_time - start_time:.4f} seconds.\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overlap(well_A: pd.DataFrame, well_B: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the percentage overlap between two horizontal wellbores.\n",
    "    \n",
    "    Parameters:\n",
    "    well_A: pd.DataFrame\n",
    "        Well trajectory data for Well A, including 'MD' (Measured Depth) and 'Inclination'.\n",
    "    well_B: pd.DataFrame\n",
    "        Well trajectory data for Well B, including 'MD' (Measured Depth) and 'Inclination'.\n",
    "    \n",
    "    Returns:\n",
    "    float:\n",
    "        Percentage of overlap relative to the shorter lateral.\n",
    "    \"\"\"\n",
    "    if well_A.empty or well_B.empty:\n",
    "        return 0.0\n",
    "\n",
    "    start_A, end_A = well_A[\"MD\"].min(), well_A[\"MD\"].max()\n",
    "    start_B, end_B = well_B[\"MD\"].min(), well_B[\"MD\"].max()\n",
    "\n",
    "    overlap_start = max(start_A, start_B)\n",
    "    overlap_end = min(end_A, end_B)\n",
    "\n",
    "    if overlap_start >= overlap_end:\n",
    "        return 0.0\n",
    "\n",
    "    overlap_length = overlap_end - overlap_start\n",
    "    shorter_length = min(end_A - start_A, end_B - start_B)\n",
    "\n",
    "    return (overlap_length / shorter_length) * 100 if shorter_length > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Defining Functions that is used to compute Lat/Lon to UTM Co-Ordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_utm_zone(longitude: float) -> int:\n",
    "    \"\"\"\n",
    "    Determines the UTM zone based on a given longitude.\n",
    "    \"\"\"\n",
    "    return int((longitude + 180) / 6) + 1\n",
    "\n",
    "\n",
    "def batch_latlon_to_utm(lat: np.ndarray, lon: np.ndarray, utm_zone: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts arrays of latitudes and longitudes to UTM coordinates in meters for a given UTM zone.\n",
    "    \"\"\"\n",
    "    proj_utm = pyproj.Transformer.from_crs(\n",
    "        \"EPSG:4326\", f\"EPSG:326{utm_zone}\", always_xy=True\n",
    "    )\n",
    "    \n",
    "    return proj_utm.transform(lon, lat)\n",
    "\n",
    "\n",
    "def compute_utm_coordinates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes UTM (x, y, z) coordinates for multiple wells, using surface location to determine UTM zones.\n",
    "    Converts UTM coordinates from meters to feet. Uses vectorized batch processing for performance.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Original directional survey DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with all original columns + x, y, z (in feet), and utm_zone.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timing\n",
    "\n",
    "    # Step 1: Sort dataframe by md to identify surface location\n",
    "    df = df.sort_values(by=[\"ChosenID\", \"md\"], ascending=[True, True])\n",
    "    \n",
    "    # Step 2: Determine UTM zones using the surface location (first row per well)\n",
    "    surface_locs = df.groupby(\"ChosenID\").first()[[\"latitude\", \"longitude\"]]\n",
    "    surface_locs[\"utm_zone\"] = surface_locs[\"longitude\"].apply(determine_utm_zone)\n",
    "\n",
    "    # Merge UTM zones back into the original dataframe\n",
    "    df = df.merge(surface_locs[[\"utm_zone\"]], on=\"ChosenID\", how=\"left\")\n",
    "\n",
    "    print(f\"âœ… Determined UTM zones in {time.time() - start_time:.4f} seconds.\")\n",
    "\n",
    "    # Step 3: Batch transformation for each unique UTM zone\n",
    "    start_transform_time = time.time()\n",
    "    unique_zones = df[\"utm_zone\"].unique()\n",
    "    utm_converters: Dict[int, Tuple[np.ndarray, np.ndarray]] = {}\n",
    "\n",
    "    for zone in unique_zones:\n",
    "        subset = df[df[\"utm_zone\"] == zone]\n",
    "        easting, northing = batch_latlon_to_utm(subset[\"latitude\"].values, subset[\"longitude\"].values, zone)\n",
    "        utm_converters[zone] = (easting, northing)\n",
    "\n",
    "    print(f\"âœ… Performed batch EPSG transformations in {time.time() - start_transform_time:.4f} seconds.\")\n",
    "\n",
    "    # Step 4: Assign the converted coordinates back to the DataFrame\n",
    "    start_assign_time = time.time()\n",
    "    df[\"x\"], df[\"y\"] = np.zeros(len(df)), np.zeros(len(df))\n",
    "\n",
    "    for zone in unique_zones:\n",
    "        mask = df[\"utm_zone\"] == zone\n",
    "        df.loc[mask, \"x\"], df.loc[mask, \"y\"] = utm_converters[zone]\n",
    "\n",
    "    print(f\"âœ… Assigned transformed coordinates in {time.time() - start_assign_time:.4f} seconds.\")\n",
    "\n",
    "    # Step 5: Convert UTM coordinates from meters to feet (Conversion factor: 1 meter = 3.28084 feet)\n",
    "    df[\"x\"] *= 3.28084\n",
    "    df[\"y\"] *= 3.28084\n",
    "    \n",
    "    df[\"z\"] = -df[\"tvd\"] # Elevation is negative TVD\n",
    "\n",
    "    print(f\"âœ… Total execution time: {time.time() - start_time:.4f} seconds.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testinig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\apoorva.saxena\\onedrive - sitio royalties\\desktop\\project - apoorva\\python\\parent_child_spacing\\src\\database_manager.py:85: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result_df = pd.read_sql(sql_query, self.connection)\n"
     ]
    }
   ],
   "source": [
    "# Importing Directional Survey data from Databricks\n",
    "\n",
    "databricks = DatabricksOdbcConnector()\n",
    "\n",
    "# Filtering only Horizontal wells and getting their apis\n",
    "chosen_ids = \", \".join(f\"'{id}'\" for id in df_raw[df_raw['HoleDirection']=='H']['ChosenID'].unique())\n",
    "\n",
    "try:\n",
    "    databricks.connect()\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        LEFT(uwi, 10) AS ChosenID, \n",
    "        station_md_uscust AS md, \n",
    "        station_tvd_uscust AS tvd,\n",
    "        inclination, \n",
    "        azimuth, \n",
    "        latitude, \n",
    "        longitude, \n",
    "        x_offset_uscust AS `deviation_E/W`,\n",
    "        ew_direction,\n",
    "        y_offset_uscust AS `deviation_N/S`,\n",
    "        ns_direction\n",
    "        \n",
    "    FROM ihs_sp.well.well_directional_survey_station\n",
    "    WHERE LEFT(uwi, 10) IN ({chosen_ids})\n",
    "    ORDER BY uwi, md;\n",
    "    \"\"\"\n",
    "\n",
    "    df_directional = databricks.execute_query(query)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    databricks.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Determined UTM zones in 0.6978 seconds.\n",
      "âœ… Performed batch EPSG transformations in 0.2321 seconds.\n",
      "âœ… Assigned transformed coordinates in 0.0158 seconds.\n",
      "âœ… Total execution time: 0.9622 seconds.\n"
     ]
    }
   ],
   "source": [
    "df_with_utm = compute_utm_coordinates(df_directional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Step 1: Trajectory DataFrame conversion took 0.0999 seconds.\n",
      "âš ï¸ The following ChosenIDs do not exist in the trajectory data and will be excluded: ['4238935759', '4238935783', '4238935872', '4238933169', '4238940091', '4238937586', '4238910461', '4238934764', '4238930501', '4238932256', '4238932191', '4238932194', '4238910497', '4238937713', '4238933558', '4238930262', '4238933989', '4238931100', '4238937707', '4238910420', '4238939527', '4238940159', '4238940321', '4238940597', '4238940596', '4238940594', '4238940625']\n",
      "âœ… Step 2: ChosenID validation and filtering took 0.0020 seconds.\n",
      "âœ… Step 3: i-k pair generation took 0.6069 seconds.\n",
      "âœ… Step 4 (Optimized): Heel/Toe extraction took 16.2367 seconds.\n",
      "âœ… Step 5: Heel/Toe dictionary lookup took 4.9940 seconds.\n",
      "âœ… Step 6 (Optimized): Distance calculations took 4.4364 seconds.\n",
      "âœ… Step 7: Drill direction calculation took 149.6608 seconds.\n",
      "âœ… Step 8: Directional relationship calculation took 0.1172 seconds.\n",
      "ðŸš€ Total Execution Time: 177.0338 seconds.\n"
     ]
    }
   ],
   "source": [
    "df_ik_pairs = create_i_k_pairs(df=df_raw, trajectories=df_with_utm[df_with_utm['inclination']>=89][['ChosenID','md','tvd','inclination','azimuth','latitude','longitude','x','y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ik_pairs.to_csv('i_k_pairs_Reeves_v1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
